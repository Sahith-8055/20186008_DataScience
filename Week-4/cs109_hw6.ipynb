{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "cs109_hw6.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua1jqIuaJDxf",
        "colab_type": "text"
      },
      "source": [
        "# CS 109A/STAT 121A/AC 209A/CSCI E-109A: Homework 6\n",
        "# Reg-Logistic Regression, ROC, and Data Imputation\n",
        "\n",
        "**Harvard University**<br/>\n",
        "**Fall 2017**<br/>\n",
        "**Instructors**: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine\n",
        "\n",
        "---\n",
        "\n",
        "### INSTRUCTIONS\n",
        "\n",
        "- To submit your assignment follow the instructions given in canvas.\n",
        "- Restart the kernel and run the whole notebook again before you submit. \n",
        "- Do not include your name(s) in the notebook if you are submitting as a group. \n",
        "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c39JfzKYJDxg",
        "colab_type": "text"
      },
      "source": [
        "Your partner's name (if you submit separately):\n",
        "\n",
        "Enrollment Status (109A, 121A, 209A, or E109A):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh14yBTxJDxh",
        "colab_type": "text"
      },
      "source": [
        "Import libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hz1ZndcJDxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "import seaborn.apionly as sns\n",
        "from sklearn.tree import export_graphviz\n",
        "from IPython.display import Image\n",
        "from IPython.display import display\n",
        "%matplotlib inline\n",
        "\n",
        "def leppard(source_data, prediction_data):\n",
        "    false_negative = 0\n",
        "    false_positive = 0\n",
        "    correct_assessment = 0\n",
        "    for result in range(0, len(prediction_data)):\n",
        "        if int(prediction_data[result]) == 1 and int(source_data[result]) == 0:\n",
        "            false_positive += 1\n",
        "        if int(prediction_data[result]) == 0 and int(source_data[result]) == 1:\n",
        "            false_negative += 1\n",
        "        if (int(prediction_data[result]) == 1 and int(source_data[result]) == 1) or (int(prediction_data[result]) == 0 and int(source_data[result]) == 0):\n",
        "            correct_assessment += 1\n",
        "    print ()\n",
        "    print (\"False Positives: \", false_positive)\n",
        "    print (\"False Negatives: \", false_negative)\n",
        "    print (\"Correct Assessment: \", correct_assessment)\n",
        "\n",
        "    print (\"Classification Accuracy: \", 1 - (false_positive + false_negative) / len(source_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG_FeJ8oJDxo",
        "colab_type": "text"
      },
      "source": [
        "## Automated Breast Cancer Detection\n",
        "\n",
        "In this homework, we will consider the problem of early breast cancer detection from X-ray images. Specifically, given a candidate region of interest (ROI) from an X-ray image of a patient's breast, the goal is to predict if the region corresponds to a malignant tumor (label 1) or is normal (label 0). The training and test data sets for this problem is provided in the file `hw6_dataset.csv`. Each row in these files corresponds to a ROI in a patient's X-ray, with columns 1-117 containing features computed using standard image processing algorithms. The last column contains the class label, and is based on a radiologist's opinion or a biopsy. This data was obtained from the KDD Cup 2008 challenge.\n",
        "\n",
        "The data set contain a total of 69,098 candidate ROIs, of which only 409 are malignant, while the remaining are all normal. \n",
        "\n",
        "*Note*: be careful of reading/treating column names and row names in this data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Np9SgTxJDxp",
        "colab_type": "text"
      },
      "source": [
        "## Question 1: Beyond Classification Accuracy\n",
        "\n",
        "\n",
        "0.  Split the data set into a training set and a testing set.  The training set should be 75% of the original data set, and the testing set 25%.  Use `np.random.seed(9001)`.\n",
        "\n",
        "1. Fit a logistic regression classifier to the training set and report the  accuracy of the classifier on the test set. You should use $L_2$ regularization in logistic regression, with the regularization parameter tuned using cross-validation. \n",
        "    1. How does the fitted model compare with a classifier that predicts 'normal' (label 0) on all patients? \n",
        "    2. Do you think the difference in the classification accuracies are large enough to declare logistic regression as a better classifier than the all 0's classifier? Why or why not?\n",
        "    \n",
        "For applications with imbalanced class labels, in this case when there are many more healthy subjects ($Y=0$) than those with cancer ($Y=1$), the classification accuracy may not be the best metric to evaluate a classifier's performance. As an alternative, we could analyze the confusion table for the classifier. \n",
        "\n",
        "<ol start=\"3\">\n",
        "<li> Compute the confusion table for both the fitted classifier and the classifier that predicts all 0's.</li>\n",
        "<li> Using the entries of the confusion table compute the *true positive rate* and the *true negative rate* for the two classifiers. Explain what these evaluation metrics mean for the specific task of cancer detection. Based on the observed metrics, comment on whether the fitted model is better than the all 0's classifier.</li>\n",
        "<li> What is the *false positive rate* of the fitted classifier, and how is it related to its true positive and true negative rate? Why is a classifier with high false positive rate undesirable for a cancer detection task?</li>\n",
        "</ol>\n",
        "*Hint:* You may use the `metrics.confusion_matrix` function to compute the confusion matrix for a classification model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgegzS8WJk-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Logistic classifier applied to Test Set:\n",
        "\n",
        "False Positives:  0\n",
        "False Negatives:  1\n",
        "Correct Assessment:  17087\n",
        "Classification Accuracy:  0.9999414794007491\n",
        "[[16984     0]\n",
        " [    1   103]]\n",
        "\n",
        "\n",
        "Classifier that predicts all normal:\n",
        "\n",
        "False Positives:  0\n",
        "False Negatives:  104\n",
        "Correct Assessment:  16984\n",
        "Classification Accuracy:  0.9939138576779026\n",
        "[[16984     0]\n",
        " [  104     0]]\n",
        "In [ ]:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrdGFDBfJl4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "The difference between the two models is substantial in light of the number of patients. The fraction of a percent that \n",
        "the logistic scored higher translates to a number of missclassifications. Further the number of false negatives is \n",
        "decreased from 104 to 1. This is the number of people who tested negative despite having cancer, the costliest error \n",
        "for our purposes. Having a classifier that predicts all people to not have cancer surely has a high accuracy rate given \n",
        "the rarity of cancer, but it does nothing to help find those rare cases that are so critical to find. On the other extreme are\n",
        "false positives. Predicting all positive makes sure that we find all cases of cancer by predicting that everyone has it. This\n",
        "is also useless as it does nothing to screen patients. False positives in cancer can cause patient anxiety and monetary costs for \n",
        "further tests, while false negatives are lethal."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWq8Q0RY0wSz",
        "colab_type": "code",
        "outputId": "37dc9eff-64ce-46e4-bf73-6c4cde980b3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTPb6FJmJoXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split the data set into a training set and a testing set\n",
        "np.random.seed(9001)\n",
        "path = \"/content/drive/My Drive/DATA SCIENCE/hw6_dataset.csv\"\n",
        "df = pd.read_csv(path)\n",
        "msk = np.random.rand(len(df)) < 0.75\n",
        "data_train = df[msk]\n",
        "data_test = df[~msk]\n",
        "orig_columns = list(data_train.columns.values)\n",
        "new_columns = []\n",
        "for x in range(len(orig_columns) - 1):\n",
        "    #print(orig_columns[x])\n",
        "    index_of_e = orig_columns[x].index('e')\n",
        "    revised_string = orig_columns[x][:index_of_e + 4]\n",
        "    #print(revised_string)\n",
        "    converted_string = float(revised_string)\n",
        "    new_columns.append(str(converted_string))\n",
        "new_columns.append('Class Label')\n",
        "#print(new_columns)\n",
        "data_train.columns = new_columns\n",
        "data_test.columns = new_columns\n",
        "data_train.head(10)\n",
        "\n",
        "y_train = data_train['Class Label'].values\n",
        "X_train = data_train.values\n",
        "y_train = y_train.reshape(len(y_train), 1)\n",
        "\n",
        "y_test = data_test['Class Label'].values\n",
        "X_test = data_test.values\n",
        "y_test = y_test.reshape(len(y_test), 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Duhdc1vSSDTS",
        "colab_type": "code",
        "outputId": "1d3a2945-e47c-48dd-ad44-522432685e2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>-1.439999999999999891e-01</th>\n",
              "      <th>-1.429999999999999882e-01</th>\n",
              "      <th>-1.160000000000000059e-01</th>\n",
              "      <th>-1.029999999999999943e-01</th>\n",
              "      <th>2.260000000000000064e-01</th>\n",
              "      <th>2.099999999999999922e-01</th>\n",
              "      <th>-9.799999999999999822e-01</th>\n",
              "      <th>-7.800000000000000266e-01</th>\n",
              "      <th>-4.739999999999999769e-01</th>\n",
              "      <th>-4.470000000000000084e-01</th>\n",
              "      <th>-1.429999999999999882e-01.1</th>\n",
              "      <th>-5.240000000000000213e-02</th>\n",
              "      <th>2.600000000000000089e-01</th>\n",
              "      <th>2.089999999999999913e-01</th>\n",
              "      <th>-2.119999999999999940e-01</th>\n",
              "      <th>5.699999999999999512e-01</th>\n",
              "      <th>-9.389999999999999458e-01</th>\n",
              "      <th>-3.980000000000000204e-01</th>\n",
              "      <th>3.950000000000000178e-01</th>\n",
              "      <th>4.139999999999999791e-01</th>\n",
              "      <th>4.980000000000000052e-03</th>\n",
              "      <th>1.310000000000000053e+00</th>\n",
              "      <th>-9.549999999999999600e-01</th>\n",
              "      <th>7.059999999999999609e-01</th>\n",
              "      <th>9.519999999999999574e-01</th>\n",
              "      <th>8.100000000000000533e-01</th>\n",
              "      <th>1.010000000000000009e+00</th>\n",
              "      <th>-6.600000000000000311e-01</th>\n",
              "      <th>8.129999999999999449e-01</th>\n",
              "      <th>-4.289999999999999925e-01</th>\n",
              "      <th>1.540000000000000036e+00</th>\n",
              "      <th>1.710000000000000131e-01</th>\n",
              "      <th>8.249999999999999556e-01</th>\n",
              "      <th>7.550000000000000044e-01</th>\n",
              "      <th>1.499999999999999944e-01</th>\n",
              "      <th>5.050000000000000044e-01</th>\n",
              "      <th>-2.349999999999999867e-01</th>\n",
              "      <th>-2.190000000000000002e-01</th>\n",
              "      <th>-1.280000000000000027e-01</th>\n",
              "      <th>-1.030000000000000027e+00</th>\n",
              "      <th>...</th>\n",
              "      <th>4.079999999999999738e-01</th>\n",
              "      <th>-9.819999999999999840e-01</th>\n",
              "      <th>-9.859999999999999876e-01</th>\n",
              "      <th>-1.370000000000000107e-01</th>\n",
              "      <th>1.789999999999999925e-01</th>\n",
              "      <th>2.670000000000000151e-01</th>\n",
              "      <th>2.800000000000000266e-01</th>\n",
              "      <th>-1.229999999999999982e+00.1</th>\n",
              "      <th>-1.209999999999999964e+00</th>\n",
              "      <th>-1.189999999999999947e+00</th>\n",
              "      <th>4.480000000000000093e-01</th>\n",
              "      <th>1.550000000000000044e+00</th>\n",
              "      <th>1.550000000000000044e+00.1</th>\n",
              "      <th>1.550000000000000044e+00.2</th>\n",
              "      <th>3.569999999999999840e-01</th>\n",
              "      <th>-9.449999999999999512e-01</th>\n",
              "      <th>-1.370000000000000107e-01.1</th>\n",
              "      <th>-1.280000000000000027e+00</th>\n",
              "      <th>-1.409999999999999920e+00</th>\n",
              "      <th>-8.960000000000000187e-01</th>\n",
              "      <th>-6.670000000000000373e-01</th>\n",
              "      <th>-1.010000000000000009e+00</th>\n",
              "      <th>-5.600000000000000533e-01</th>\n",
              "      <th>-5.629999999999999449e-01</th>\n",
              "      <th>-5.659999999999999476e-01</th>\n",
              "      <th>-8.040000000000000480e-01</th>\n",
              "      <th>-6.179999999999999938e-01</th>\n",
              "      <th>3.440000000000000002e-02</th>\n",
              "      <th>4.259999999999999898e-01</th>\n",
              "      <th>-7.379999999999999893e-01</th>\n",
              "      <th>9.250000000000000444e-01</th>\n",
              "      <th>5.160000000000000142e-01</th>\n",
              "      <th>3.439999999999999725e-01</th>\n",
              "      <th>9.060000000000000275e-01</th>\n",
              "      <th>-1.129999999999999893e+00</th>\n",
              "      <th>-5.520000000000000462e-01</th>\n",
              "      <th>5.530000000000000471e-01</th>\n",
              "      <th>-4.169999999999999818e-01</th>\n",
              "      <th>2.560000000000000053e-01</th>\n",
              "      <th>0.000000000000000000e+00</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.01100</td>\n",
              "      <td>0.138</td>\n",
              "      <td>-0.2230</td>\n",
              "      <td>-0.1730</td>\n",
              "      <td>0.188</td>\n",
              "      <td>0.284</td>\n",
              "      <td>-0.0522</td>\n",
              "      <td>-0.256</td>\n",
              "      <td>0.129</td>\n",
              "      <td>0.427</td>\n",
              "      <td>-0.246</td>\n",
              "      <td>-0.236</td>\n",
              "      <td>-0.665</td>\n",
              "      <td>0.227</td>\n",
              "      <td>-0.248</td>\n",
              "      <td>0.658</td>\n",
              "      <td>-0.249</td>\n",
              "      <td>0.194</td>\n",
              "      <td>-0.59500</td>\n",
              "      <td>0.3780</td>\n",
              "      <td>0.636</td>\n",
              "      <td>-1.070</td>\n",
              "      <td>-0.941</td>\n",
              "      <td>-0.0361</td>\n",
              "      <td>-0.254</td>\n",
              "      <td>-0.116</td>\n",
              "      <td>-1.340</td>\n",
              "      <td>0.345</td>\n",
              "      <td>-1.400</td>\n",
              "      <td>0.356</td>\n",
              "      <td>-0.876</td>\n",
              "      <td>-0.5150</td>\n",
              "      <td>-0.4420</td>\n",
              "      <td>-0.4400</td>\n",
              "      <td>0.606</td>\n",
              "      <td>0.295</td>\n",
              "      <td>-0.555</td>\n",
              "      <td>-0.560</td>\n",
              "      <td>-0.553</td>\n",
              "      <td>-0.152</td>\n",
              "      <td>...</td>\n",
              "      <td>0.457</td>\n",
              "      <td>0.0503</td>\n",
              "      <td>0.0854</td>\n",
              "      <td>0.209</td>\n",
              "      <td>0.5240</td>\n",
              "      <td>-0.0818</td>\n",
              "      <td>-0.0935</td>\n",
              "      <td>-0.211</td>\n",
              "      <td>-0.199</td>\n",
              "      <td>-0.192</td>\n",
              "      <td>-0.3730</td>\n",
              "      <td>-0.6660</td>\n",
              "      <td>-0.6660</td>\n",
              "      <td>-0.6660</td>\n",
              "      <td>-0.3270</td>\n",
              "      <td>0.407</td>\n",
              "      <td>-0.476</td>\n",
              "      <td>-0.0575</td>\n",
              "      <td>-0.0798</td>\n",
              "      <td>0.408</td>\n",
              "      <td>0.347</td>\n",
              "      <td>1.2800</td>\n",
              "      <td>0.504</td>\n",
              "      <td>0.556</td>\n",
              "      <td>0.634</td>\n",
              "      <td>1.06000</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.0175</td>\n",
              "      <td>-0.5580</td>\n",
              "      <td>0.420</td>\n",
              "      <td>-0.593</td>\n",
              "      <td>0.452</td>\n",
              "      <td>0.00785</td>\n",
              "      <td>-0.533</td>\n",
              "      <td>-0.0789</td>\n",
              "      <td>0.705</td>\n",
              "      <td>0.906</td>\n",
              "      <td>0.216</td>\n",
              "      <td>-0.0723</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.21200</td>\n",
              "      <td>-0.313</td>\n",
              "      <td>0.2660</td>\n",
              "      <td>0.2320</td>\n",
              "      <td>-1.190</td>\n",
              "      <td>-1.150</td>\n",
              "      <td>-1.8100</td>\n",
              "      <td>-1.560</td>\n",
              "      <td>-1.250</td>\n",
              "      <td>-1.200</td>\n",
              "      <td>-0.527</td>\n",
              "      <td>-0.463</td>\n",
              "      <td>-0.975</td>\n",
              "      <td>-4.720</td>\n",
              "      <td>4.340</td>\n",
              "      <td>0.684</td>\n",
              "      <td>-1.280</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.00121</td>\n",
              "      <td>0.1100</td>\n",
              "      <td>0.142</td>\n",
              "      <td>-0.281</td>\n",
              "      <td>-0.502</td>\n",
              "      <td>2.1900</td>\n",
              "      <td>0.788</td>\n",
              "      <td>0.572</td>\n",
              "      <td>-1.100</td>\n",
              "      <td>0.878</td>\n",
              "      <td>-1.000</td>\n",
              "      <td>1.390</td>\n",
              "      <td>1.150</td>\n",
              "      <td>1.2600</td>\n",
              "      <td>0.0949</td>\n",
              "      <td>0.0709</td>\n",
              "      <td>-2.630</td>\n",
              "      <td>0.685</td>\n",
              "      <td>4.060</td>\n",
              "      <td>4.080</td>\n",
              "      <td>4.330</td>\n",
              "      <td>-0.893</td>\n",
              "      <td>...</td>\n",
              "      <td>1.320</td>\n",
              "      <td>-0.9980</td>\n",
              "      <td>-0.9940</td>\n",
              "      <td>-2.340</td>\n",
              "      <td>0.0251</td>\n",
              "      <td>-0.0267</td>\n",
              "      <td>-0.0138</td>\n",
              "      <td>-1.200</td>\n",
              "      <td>-1.180</td>\n",
              "      <td>-1.160</td>\n",
              "      <td>3.2400</td>\n",
              "      <td>-0.7540</td>\n",
              "      <td>-0.7430</td>\n",
              "      <td>-0.7360</td>\n",
              "      <td>-1.4200</td>\n",
              "      <td>-0.491</td>\n",
              "      <td>3.520</td>\n",
              "      <td>-0.2860</td>\n",
              "      <td>-1.9300</td>\n",
              "      <td>-0.415</td>\n",
              "      <td>-3.030</td>\n",
              "      <td>-0.0145</td>\n",
              "      <td>-0.666</td>\n",
              "      <td>-0.666</td>\n",
              "      <td>-0.668</td>\n",
              "      <td>-0.10700</td>\n",
              "      <td>0.184</td>\n",
              "      <td>-0.1500</td>\n",
              "      <td>0.0689</td>\n",
              "      <td>-0.158</td>\n",
              "      <td>-0.816</td>\n",
              "      <td>1.570</td>\n",
              "      <td>0.39400</td>\n",
              "      <td>1.340</td>\n",
              "      <td>-1.1800</td>\n",
              "      <td>-2.700</td>\n",
              "      <td>-0.926</td>\n",
              "      <td>-2.650</td>\n",
              "      <td>-0.0447</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.21500</td>\n",
              "      <td>-0.184</td>\n",
              "      <td>0.0274</td>\n",
              "      <td>0.0494</td>\n",
              "      <td>0.443</td>\n",
              "      <td>0.463</td>\n",
              "      <td>-1.0500</td>\n",
              "      <td>-0.941</td>\n",
              "      <td>-0.531</td>\n",
              "      <td>-0.394</td>\n",
              "      <td>-0.409</td>\n",
              "      <td>-0.103</td>\n",
              "      <td>0.461</td>\n",
              "      <td>0.219</td>\n",
              "      <td>-0.212</td>\n",
              "      <td>0.704</td>\n",
              "      <td>-0.940</td>\n",
              "      <td>-0.483</td>\n",
              "      <td>-0.14200</td>\n",
              "      <td>-0.7110</td>\n",
              "      <td>-0.544</td>\n",
              "      <td>0.804</td>\n",
              "      <td>1.490</td>\n",
              "      <td>0.7600</td>\n",
              "      <td>0.951</td>\n",
              "      <td>0.839</td>\n",
              "      <td>-0.761</td>\n",
              "      <td>0.456</td>\n",
              "      <td>-0.731</td>\n",
              "      <td>0.492</td>\n",
              "      <td>1.700</td>\n",
              "      <td>0.1830</td>\n",
              "      <td>1.2700</td>\n",
              "      <td>0.5270</td>\n",
              "      <td>-0.417</td>\n",
              "      <td>1.070</td>\n",
              "      <td>2.360</td>\n",
              "      <td>2.410</td>\n",
              "      <td>2.760</td>\n",
              "      <td>-0.970</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.699</td>\n",
              "      <td>-0.8860</td>\n",
              "      <td>-0.8960</td>\n",
              "      <td>0.536</td>\n",
              "      <td>0.8490</td>\n",
              "      <td>-0.0485</td>\n",
              "      <td>-0.0455</td>\n",
              "      <td>0.278</td>\n",
              "      <td>0.369</td>\n",
              "      <td>0.504</td>\n",
              "      <td>0.0412</td>\n",
              "      <td>0.0851</td>\n",
              "      <td>0.0905</td>\n",
              "      <td>0.0941</td>\n",
              "      <td>1.1600</td>\n",
              "      <td>0.455</td>\n",
              "      <td>1.370</td>\n",
              "      <td>0.5750</td>\n",
              "      <td>-0.9700</td>\n",
              "      <td>0.568</td>\n",
              "      <td>-0.393</td>\n",
              "      <td>-0.5110</td>\n",
              "      <td>-1.450</td>\n",
              "      <td>-1.480</td>\n",
              "      <td>-1.530</td>\n",
              "      <td>-0.38100</td>\n",
              "      <td>-1.090</td>\n",
              "      <td>-0.3030</td>\n",
              "      <td>0.2990</td>\n",
              "      <td>0.953</td>\n",
              "      <td>0.634</td>\n",
              "      <td>0.111</td>\n",
              "      <td>0.37100</td>\n",
              "      <td>0.859</td>\n",
              "      <td>-0.9930</td>\n",
              "      <td>-0.492</td>\n",
              "      <td>0.363</td>\n",
              "      <td>0.326</td>\n",
              "      <td>-0.0528</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.27900</td>\n",
              "      <td>-0.197</td>\n",
              "      <td>0.1270</td>\n",
              "      <td>0.0973</td>\n",
              "      <td>-0.213</td>\n",
              "      <td>-0.150</td>\n",
              "      <td>-1.3200</td>\n",
              "      <td>-0.994</td>\n",
              "      <td>-1.110</td>\n",
              "      <td>-1.090</td>\n",
              "      <td>-0.586</td>\n",
              "      <td>-0.818</td>\n",
              "      <td>1.180</td>\n",
              "      <td>0.242</td>\n",
              "      <td>-0.228</td>\n",
              "      <td>1.550</td>\n",
              "      <td>-0.306</td>\n",
              "      <td>-0.261</td>\n",
              "      <td>0.04740</td>\n",
              "      <td>-1.8700</td>\n",
              "      <td>-1.770</td>\n",
              "      <td>0.886</td>\n",
              "      <td>-0.955</td>\n",
              "      <td>0.2180</td>\n",
              "      <td>0.926</td>\n",
              "      <td>0.967</td>\n",
              "      <td>0.456</td>\n",
              "      <td>-0.882</td>\n",
              "      <td>0.280</td>\n",
              "      <td>-0.682</td>\n",
              "      <td>2.180</td>\n",
              "      <td>1.4100</td>\n",
              "      <td>-0.0310</td>\n",
              "      <td>0.0381</td>\n",
              "      <td>-1.250</td>\n",
              "      <td>0.731</td>\n",
              "      <td>-0.632</td>\n",
              "      <td>-0.625</td>\n",
              "      <td>-0.587</td>\n",
              "      <td>-1.220</td>\n",
              "      <td>...</td>\n",
              "      <td>2.090</td>\n",
              "      <td>-0.9270</td>\n",
              "      <td>-0.9220</td>\n",
              "      <td>-0.637</td>\n",
              "      <td>-0.0210</td>\n",
              "      <td>0.4820</td>\n",
              "      <td>0.5320</td>\n",
              "      <td>-1.380</td>\n",
              "      <td>-1.380</td>\n",
              "      <td>-1.390</td>\n",
              "      <td>0.5800</td>\n",
              "      <td>1.3300</td>\n",
              "      <td>1.3300</td>\n",
              "      <td>1.3200</td>\n",
              "      <td>-0.0827</td>\n",
              "      <td>-0.903</td>\n",
              "      <td>0.551</td>\n",
              "      <td>-1.4500</td>\n",
              "      <td>-1.4000</td>\n",
              "      <td>-0.853</td>\n",
              "      <td>-0.784</td>\n",
              "      <td>-0.6930</td>\n",
              "      <td>-0.236</td>\n",
              "      <td>-0.210</td>\n",
              "      <td>-0.174</td>\n",
              "      <td>-0.86900</td>\n",
              "      <td>0.222</td>\n",
              "      <td>0.2760</td>\n",
              "      <td>1.0400</td>\n",
              "      <td>-0.688</td>\n",
              "      <td>-0.640</td>\n",
              "      <td>0.485</td>\n",
              "      <td>0.29500</td>\n",
              "      <td>0.403</td>\n",
              "      <td>-1.1200</td>\n",
              "      <td>-0.343</td>\n",
              "      <td>0.468</td>\n",
              "      <td>-0.820</td>\n",
              "      <td>0.4350</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00922</td>\n",
              "      <td>-0.138</td>\n",
              "      <td>0.1690</td>\n",
              "      <td>0.1540</td>\n",
              "      <td>-0.391</td>\n",
              "      <td>-0.397</td>\n",
              "      <td>-1.6900</td>\n",
              "      <td>-1.450</td>\n",
              "      <td>-0.546</td>\n",
              "      <td>-0.527</td>\n",
              "      <td>-0.586</td>\n",
              "      <td>-0.835</td>\n",
              "      <td>-0.186</td>\n",
              "      <td>0.237</td>\n",
              "      <td>-0.225</td>\n",
              "      <td>1.590</td>\n",
              "      <td>-1.090</td>\n",
              "      <td>-0.421</td>\n",
              "      <td>0.30600</td>\n",
              "      <td>0.0257</td>\n",
              "      <td>-0.090</td>\n",
              "      <td>1.310</td>\n",
              "      <td>-0.955</td>\n",
              "      <td>1.5400</td>\n",
              "      <td>0.704</td>\n",
              "      <td>0.535</td>\n",
              "      <td>-0.380</td>\n",
              "      <td>-0.167</td>\n",
              "      <td>-0.478</td>\n",
              "      <td>-0.104</td>\n",
              "      <td>0.675</td>\n",
              "      <td>0.0871</td>\n",
              "      <td>0.5390</td>\n",
              "      <td>0.4150</td>\n",
              "      <td>-1.350</td>\n",
              "      <td>1.310</td>\n",
              "      <td>0.736</td>\n",
              "      <td>0.754</td>\n",
              "      <td>0.852</td>\n",
              "      <td>-1.170</td>\n",
              "      <td>...</td>\n",
              "      <td>0.772</td>\n",
              "      <td>-0.8860</td>\n",
              "      <td>-0.8780</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.8350</td>\n",
              "      <td>0.1650</td>\n",
              "      <td>0.1740</td>\n",
              "      <td>-1.110</td>\n",
              "      <td>-1.070</td>\n",
              "      <td>-1.030</td>\n",
              "      <td>0.3780</td>\n",
              "      <td>0.9880</td>\n",
              "      <td>0.9920</td>\n",
              "      <td>0.9950</td>\n",
              "      <td>0.6370</td>\n",
              "      <td>0.447</td>\n",
              "      <td>0.426</td>\n",
              "      <td>-1.1000</td>\n",
              "      <td>-1.2200</td>\n",
              "      <td>0.562</td>\n",
              "      <td>-0.752</td>\n",
              "      <td>0.0562</td>\n",
              "      <td>-0.412</td>\n",
              "      <td>-0.414</td>\n",
              "      <td>-0.418</td>\n",
              "      <td>-0.00122</td>\n",
              "      <td>-0.236</td>\n",
              "      <td>0.1580</td>\n",
              "      <td>0.1580</td>\n",
              "      <td>0.940</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>0.699</td>\n",
              "      <td>0.37100</td>\n",
              "      <td>0.481</td>\n",
              "      <td>-1.0600</td>\n",
              "      <td>-0.526</td>\n",
              "      <td>0.550</td>\n",
              "      <td>-0.284</td>\n",
              "      <td>0.1550</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 118 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   -1.439999999999999891e-01  ...  0.000000000000000000e+00\n",
              "0                   -0.01100  ...                       0.0\n",
              "1                    0.21200  ...                       0.0\n",
              "2                    0.21500  ...                       0.0\n",
              "3                    0.27900  ...                       0.0\n",
              "4                    0.00922  ...                       0.0\n",
              "\n",
              "[5 rows x 118 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gI3NPOHxSLF7",
        "colab_type": "code",
        "outputId": "190eda01-9a53-495a-8ddb-fdc4a53a2811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Fit a logistic regression classifier to the training set and report the accuracy of the classifier on the test set\n",
        "clf = LogisticRegressionCV(\n",
        "        Cs=list(np.power(10.0, np.arange(-10, 10)))\n",
        "        ,penalty='l2'\n",
        "        ,cv=10\n",
        "        ,random_state=777\n",
        "        ,fit_intercept=True\n",
        "        ,solver='newton-cg'\n",
        "        ,tol=10)\n",
        "clf.fit(X_train, y_train)\n",
        "print('\\n')\n",
        "print(\"The optimized L2 regularization paramater id:\", clf.C_)\n",
        "\n",
        "# The coefficients\n",
        "print('Estimated beta1: \\n', clf.coef_)\n",
        "print('Estimated beta0: \\n', clf.intercept_)\n",
        "\n",
        "# Scoring\n",
        "clf_y_pred_test = clf.predict(X_test)\n",
        "clf_y_pred_test = clf_y_pred_test.reshape(len(clf_y_pred_test), 1)\n",
        "test_df = pd.DataFrame(clf_y_pred_test)\n",
        "Total = test_df[0].sum()\n",
        "print('\\n')\n",
        "print(\"malignant: \", Total)\n",
        "\n",
        "pd.set_option('display.max_rows', 1000)\n",
        "test_df['All Normal'] = 0\n",
        "\n",
        "# Reset indexes so copy will work\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "data_test = data_test.reset_index(drop=True)\n",
        "test_df['Class Label'] = data_test['Class Label']\n",
        "\n",
        "# Confusion Matrix\n",
        "print('\\n')\n",
        "print('Classifier applied to Test Set:') \n",
        "leppard(test_df['Class Label'], test_df[0])\n",
        "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
        "\n",
        "\n",
        "print('\\n')\n",
        "print('Classifier that predicts all normal:')\n",
        "leppard(test_df['Class Label'], test_df['All Normal'])\n",
        "print(confusion_matrix(y_test, test_df['All Normal']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "The optimized L2 regularization paramater id: [10.]\n",
            "Estimated beta1: \n",
            " [[-2.15675788e-02  4.57672275e-02  5.87118921e-01  5.01597817e-01\n",
            "  -2.54420822e-01 -2.18567421e-01  1.68100075e-01 -7.42715910e-02\n",
            "   1.15897510e-01 -3.84389605e-02 -5.24469245e-02  1.31078432e-01\n",
            "  -2.18614608e-01  1.63000844e-02  2.81643912e-02 -1.40624704e-02\n",
            "  -7.47535835e-02 -7.52547764e-02 -1.01655861e-01 -9.82645388e-02\n",
            "   1.57753724e-01  1.90654339e-02  8.81488343e-02 -3.22963533e-02\n",
            "   2.91600411e-01 -5.44620449e-02  2.61948107e-02 -9.40194847e-02\n",
            "  -1.88076478e-02 -3.40947734e-02 -4.57707559e-02  7.21185951e-02\n",
            "   2.34914798e-01  8.33270689e-02  4.96411513e-02 -6.72526580e-02\n",
            "   6.36390717e-02  5.50162171e-02 -1.41451795e-02  1.01859319e-02\n",
            "   1.31628139e-02  1.98233744e-02  2.33209292e-02 -7.50357231e-02\n",
            "   2.06580734e-02  3.89938929e-02  5.63545230e-02 -1.72732061e-01\n",
            "  -1.56078561e-01 -6.21999405e-02 -1.59293848e-01  1.10459116e-01\n",
            "  -4.91734405e-02 -3.82534674e-02 -3.06234664e-02 -2.53074442e-02\n",
            "  -1.69751519e-02  2.64366844e-01 -2.61942751e-01  7.40221217e-02\n",
            "  -5.09557906e-02  7.36473448e-02  7.21481064e-02  8.75476258e-02\n",
            "   9.49586328e-02  1.34815237e-02  2.39635125e-01 -1.65466526e-01\n",
            "  -1.22648929e-02  1.44632563e-02 -3.08337994e-02  3.58424516e-02\n",
            "   1.01931071e-01  1.34155160e-02  1.07915139e-02  8.49929922e-03\n",
            "   3.31735456e-02  8.11659605e-02  1.02843686e-02  7.39264394e-02\n",
            "   7.17942220e-02 -3.67858320e-02 -3.15508923e-02 -6.81643539e-03\n",
            "   3.21578240e-02  3.28756310e-02  6.09449453e-02  1.18699475e-01\n",
            "   1.63958796e-01 -4.67669831e-02 -5.25847436e-02 -5.38053051e-02\n",
            "  -1.15033903e-01 -4.30447486e-02  3.07031774e-02 -2.42748770e-01\n",
            "  -6.27315507e-02 -1.81842207e-02 -1.08988785e-02  1.19280921e-01\n",
            "   1.30252952e-01  6.46796947e-02 -1.83790310e-01 -4.91054535e-02\n",
            "   1.48517047e-02 -1.34156343e-02 -7.81414910e-02  1.96245523e-02\n",
            "   7.85621662e-02 -6.15018887e-02 -8.59113356e-02 -2.00210066e-01\n",
            "  -5.88035791e-02 -7.06945540e-02  8.43176810e-03  7.70495819e-02\n",
            "  -5.77997456e-02  1.12553641e+01]]\n",
            "Estimated beta0: \n",
            " [-9.51671295]\n",
            "\n",
            "\n",
            "malignant:  103.0\n",
            "\n",
            "\n",
            "Classifier applied to Test Set:\n",
            "\n",
            "False Positives:  0\n",
            "False Negatives:  1\n",
            "Correct Assessment:  17087\n",
            "Classification Accuracy:  0.9999414794007491\n",
            "[[16984     0]\n",
            " [    1   103]]\n",
            "\n",
            "\n",
            "Classifier that predicts all normal:\n",
            "\n",
            "False Positives:  0\n",
            "False Negatives:  104\n",
            "Correct Assessment:  16984\n",
            "Classification Accuracy:  0.9939138576779026\n",
            "[[16984     0]\n",
            " [  104     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDtlAggQ2h1q",
        "colab_type": "code",
        "outputId": "f10d5a5b-053c-45de-9afe-6f80917ce3fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "def t_repredict(est, t, xtest):\n",
        "    probs = est.predict_proba(xtest)\n",
        "    p0 = probs[:,0]\n",
        "    p1 = probs[:,1]\n",
        "    ypred = (p1 > t)*1\n",
        "    return ypred\n",
        "print('Confusion matrix that predicts all patients to be negative:')\n",
        "print(confusion_matrix(y_test,t_repredict(clf, 01.00, X_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix that predicts all patients to be negative:\n",
            "[[16984     0]\n",
            " [  104     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgtAHQftJDxq",
        "colab_type": "text"
      },
      "source": [
        "## Question 2: ROC Analysis\n",
        "\n",
        "Another powerful diagnostic tool for class-imbalanced classification tasks is the Receiver Operating Characteristic (ROC) curve. Notice that the default logistic regression classifier in `sklearn` classifies a data point by thresholding the predicted class probability $\\hat{P}(Y=1)$ at 0.5. By using a different threshold, we can adjust the trade-off between the true positive rate (TPR) and false positive rate (FPR) of the classifier. The ROC curve allows us to visualize this trade-off across all possible thresholds.\n",
        "\n",
        "\n",
        "1. Display the ROC curve for the fitted classifier on the *test set*. In the same plot, also display the ROC curve for the all 0's classifier. How do the two curves compare?\n",
        "\n",
        "2.  Compute the highest TPR that can be achieved by the classifier at each of the following FPR's, and the thresholds at which they are achieved. Based on your results, comment on how the threshold influences a classifier's FPR.\n",
        "    - FPR = 0\n",
        "    - FPR = 0.1\n",
        "    - FPR = 0.5\n",
        "    - FPR = 0.9\n",
        "- Suppose a clinician told you that diagnosing a cancer patient as normal is *twice* as critical an error as diagnosing a normal patient as having cancer. Based on this information, what threshold would you recommend the clinician to use? What is the TPR and FPR of the classifier at this threshold? \n",
        "\n",
        "- Compute the area under the ROC curve (AUC) for both the fitted classifier and the all 0's classifier. How does the difference in the AUCs of the two classifiers compare with the difference between their classification accuracies in Question 1, Part 2(A)? \n",
        "\n",
        "*Hint:* You may use the `metrics.roc_curve` function to compute the ROC curve for a classification model and the `metrics.roc_auc_score` function to compute the AUC for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP-Y4RUX2qH2",
        "colab_type": "code",
        "outputId": "90e35ca7-1952-4bc7-876f-96e27716df6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "def make_roc(name, clf, ytest, xtest, ax=None, labe=5, proba=True, skip=0):\n",
        "    initial=False\n",
        "    if not ax:\n",
        "        ax=plt.gca()\n",
        "        initial=True\n",
        "    if proba:#for stuff like logistic regression\n",
        "        fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n",
        "    else:#for stuff like SVM\n",
        "        fpr, tpr, thresholds=roc_curve(ytest, clf.decision_function(xtest))\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    if skip:\n",
        "        l=fpr.shape[0]\n",
        "        ax.plot(fpr[0:l:skip], tpr[0:l:skip], '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
        "    else:\n",
        "        ax.plot(fpr, tpr, '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
        "    label_kwargs = {}\n",
        "    label_kwargs['bbox'] = dict(\n",
        "        boxstyle='round,pad=0.3', alpha=0.2,\n",
        "    )\n",
        "    if labe!=None:    \n",
        "        for k in range(0, fpr.shape[0],labe):\n",
        "            threshold = str(np.round(thresholds[k], 2))\n",
        "            ax.annotate(threshold, (fpr[k], tpr[k]), **label_kwargs)\n",
        "    if initial:\n",
        "        ax.set_xlim([0.0, 1.0])\n",
        "        ax.set_ylim([0.0, 1.05])\n",
        "        ax.set_xlabel('False Positive Rate')\n",
        "        ax.set_ylabel('True Positive Rate')\n",
        "        ax.set_title('ROC')\n",
        "    fpr_0, tpr_0, thresholds_1 = metrics.roc_curve(y_test, t_repredict(clf, 01.00, X_test))\n",
        "    roc_auc_0 = auc(fpr_0, tpr_0)\n",
        "    plt.plot(fpr_0, tpr_0, '.-', alpha=0.3, label='ROC curve for all Negative Predictions (area = %0.2f)' % (roc_auc_0))\n",
        "    ax.legend(loc=\"lower right\")\n",
        "    return ax\n",
        "\n",
        "ax=make_roc(\"logistic\",clf, y_test, X_test, labe=100, skip=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxBOiAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnkSm-Gk20gl",
        "colab_type": "code",
        "outputId": "ae037bc7-c4a4-4799-bbf7-ccb9d879d14f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(clf.predict_proba(X_test)[:,1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17088"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV61NeyL24H_",
        "colab_type": "code",
        "outputId": "1befd8c5-a959-49ac-a0c2-cac2649d982e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "source": [
        "fprs = [0,.1,.5,.9]\n",
        "fpr, tpr, thresholds=roc_curve(y_test, clf.predict_proba(X_test)[:,1])       \n",
        "for i in range(len(fpr)):        \n",
        "    if int(fpr[i] > 0):\n",
        "                print('FPR:', fpr[i], 'TPR', tpr[i], 'Threshold', thresholds[i] )\n",
        "                break\n",
        "for i in range(len(fpr)):             \n",
        "    if int(fpr[i]) >= .1:\n",
        "                print('FPR:', fpr[i], 'TPR', tpr[i], 'Threshold', thresholds[i] )\n",
        "                \n",
        "for i in range(len(fpr)):                 \n",
        "    if int(fpr[i]) >= .5:\n",
        "                print('FPR:', fpr[i], 'TPR', tpr[i], 'Threshold', thresholds[i] ) \n",
        "                \n",
        "for i in range(len(fpr)):      \n",
        "    if int(fpr[i]) >= .9:\n",
        "                print('FPR:', fpr[i], 'TPR', tpr[i], 'Threshold', thresholds[i] )  \n",
        "                \n",
        "                f, ax = plt.subplots()\n",
        "                ax.plot(fpr, tpr)\n",
        "                ax.set_xlabel('False Positive Rate')\n",
        "                ax.set_ylabel('True Positive Rate')\n",
        "                ax.set_title('ROC')\n",
        "                ax.legend(loc=\"lower right\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "FPR: 0.03144135657089025 TPR 1.0 Threshold 0.0005805368104031581\n",
            "FPR: 1.0 TPR 1.0 Threshold 8.233907077829796e-37\n",
            "FPR: 1.0 TPR 1.0 Threshold 8.233907077829796e-37\n",
            "FPR: 1.0 TPR 1.0 Threshold 8.233907077829796e-37\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIEnqGAqSpM7/BwIyKjhzsxx8AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1Cee4w7277K",
        "colab_type": "code",
        "outputId": "1c7bea2b-4ada-48e9-e85a-5c794ad0536b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('fprs:',fpr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fprs: [0.         0.         0.         0.03144136 0.03155911 0.13565709\n",
            " 0.13577485 0.65667687 0.65679463 0.84979981 0.84991757 1.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1sFuDIs29CN",
        "colab_type": "code",
        "outputId": "dd92846f-a2ea-48cf-b137-a42c5edacd51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('tprs:',tpr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tprs: [0.         0.00961538 1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NExIxVyL2_ba",
        "colab_type": "code",
        "outputId": "b103a92c-8eb5-4f49-ccb2-e3eaf0e40e77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print('thresholds:',thresholds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "thresholds: [1.99955010e+00 9.99550103e-01 4.40607366e-01 5.80536810e-04\n",
            " 5.80130268e-04 2.67749911e-04 2.67700947e-04 5.08769907e-05\n",
            " 5.08615956e-05 2.21872169e-05 2.21809413e-05 8.23390708e-37]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "AeqAHGdRJDxr",
        "colab_type": "text"
      },
      "source": [
        "## Question 3: Missing data\n",
        "\n",
        "In this problem you are given a different data set, `hw6_dataset_missing.csv`, that is  similar to the one you used above (same column definitions and same conditions), however this data set contains missing values. \n",
        "\n",
        "*Note*: be careful of reading/treating column names and row names in this data set as well, it *may* be different than the first data set.\n",
        "\n",
        "\n",
        "1. Remove all observations that contain and missing values, split the dataset into a 75-25 train-test split, and fit the regularized logistic regression as in Question 1 (use `LogisticRegressionCV` again to retune).  Report the overall classification rate and TPR in the test set.\n",
        "2. Restart with a fresh copy of the data in `hw6_dataset_missing.csv` and impute the missing data via mean imputation.  Split the data 75-25 and fit the regularized logistic regression model.  Report the overall classification rate and TPR in the test set.  \n",
        "3. Again restart with a fresh copy of the data in `hw6_dataset_missing.csv` and impute the missing data via a model-based imputation method. Once again split the data 75-25 and fit the regularized logistic regression model.  Report the overall classification rate and TPR in the test set.  \n",
        "4. Compare the results in the 3 previous parts of this problem.  Prepare a paragraph (5-6 sentences) discussing the results, the computational complexity of the methods, and conjecture and explain why you get the results that you see.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk-7KOqY3C5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(9001)\n",
        "path = \"/content/drive/My Drive/DATA SCIENCE/HW6_dataset_missing.csv\"\n",
        "df = pd.read_csv(path)\n",
        "msk = np.random.rand(len(df)) < 0.75\n",
        "data_train = df[msk]\n",
        "data_test = df[~msk]\n",
        "data_train = data_train.dropna()\n",
        "data_test = data_test.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI7f2YCI3Psi",
        "colab_type": "code",
        "outputId": "a0b9233f-0456-4792-9e2b-b32661876e1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.1290</td>\n",
              "      <td>-0.2160</td>\n",
              "      <td>0.2880</td>\n",
              "      <td>0.2370</td>\n",
              "      <td>-0.993</td>\n",
              "      <td>-0.9550</td>\n",
              "      <td>-1.620</td>\n",
              "      <td>-1.470</td>\n",
              "      <td>-1.0100</td>\n",
              "      <td>-1.0100</td>\n",
              "      <td>-0.187</td>\n",
              "      <td>0.323</td>\n",
              "      <td>-0.850</td>\n",
              "      <td>-3.700</td>\n",
              "      <td>4.160</td>\n",
              "      <td>3.8900</td>\n",
              "      <td>-1.150</td>\n",
              "      <td>0.162</td>\n",
              "      <td>0.0996</td>\n",
              "      <td>0.315</td>\n",
              "      <td>0.1480</td>\n",
              "      <td>0.3690</td>\n",
              "      <td>1.040</td>\n",
              "      <td>2.66000</td>\n",
              "      <td>0.96500</td>\n",
              "      <td>0.8820</td>\n",
              "      <td>1.260</td>\n",
              "      <td>-0.3750</td>\n",
              "      <td>1.020</td>\n",
              "      <td>-0.1160</td>\n",
              "      <td>1.0500</td>\n",
              "      <td>1.1900</td>\n",
              "      <td>-0.1130</td>\n",
              "      <td>0.262</td>\n",
              "      <td>-2.690</td>\n",
              "      <td>0.9160</td>\n",
              "      <td>2.6800</td>\n",
              "      <td>2.7200</td>\n",
              "      <td>3.0100</td>\n",
              "      <td>...</td>\n",
              "      <td>0.827</td>\n",
              "      <td>-1.070</td>\n",
              "      <td>-1.080</td>\n",
              "      <td>-1.700</td>\n",
              "      <td>0.0619</td>\n",
              "      <td>0.0313</td>\n",
              "      <td>0.0462</td>\n",
              "      <td>-1.3400</td>\n",
              "      <td>-1.3300</td>\n",
              "      <td>-1.3200</td>\n",
              "      <td>2.960</td>\n",
              "      <td>-1.090</td>\n",
              "      <td>-1.080</td>\n",
              "      <td>-1.0800</td>\n",
              "      <td>-0.2720</td>\n",
              "      <td>-0.224</td>\n",
              "      <td>2.330</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-2.1000</td>\n",
              "      <td>-0.131</td>\n",
              "      <td>-3.010</td>\n",
              "      <td>-0.950</td>\n",
              "      <td>-0.578</td>\n",
              "      <td>-0.576</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.952</td>\n",
              "      <td>0.1830</td>\n",
              "      <td>-0.084</td>\n",
              "      <td>0.173</td>\n",
              "      <td>0.186</td>\n",
              "      <td>-1.1900</td>\n",
              "      <td>1.1000</td>\n",
              "      <td>0.395</td>\n",
              "      <td>2.060</td>\n",
              "      <td>-1.180</td>\n",
              "      <td>-2.8500</td>\n",
              "      <td>-1.290</td>\n",
              "      <td>-2.100</td>\n",
              "      <td>0.0121</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0989</td>\n",
              "      <td>0.1160</td>\n",
              "      <td>0.3130</td>\n",
              "      <td>0.2810</td>\n",
              "      <td>-0.188</td>\n",
              "      <td>-0.2790</td>\n",
              "      <td>0.173</td>\n",
              "      <td>0.445</td>\n",
              "      <td>0.4320</td>\n",
              "      <td>0.9440</td>\n",
              "      <td>-0.468</td>\n",
              "      <td>-0.489</td>\n",
              "      <td>0.655</td>\n",
              "      <td>0.258</td>\n",
              "      <td>-0.247</td>\n",
              "      <td>0.3590</td>\n",
              "      <td>0.449</td>\n",
              "      <td>-0.490</td>\n",
              "      <td>0.2410</td>\n",
              "      <td>0.338</td>\n",
              "      <td>0.7960</td>\n",
              "      <td>1.5800</td>\n",
              "      <td>0.602</td>\n",
              "      <td>-0.41000</td>\n",
              "      <td>-0.94900</td>\n",
              "      <td>-0.9700</td>\n",
              "      <td>0.360</td>\n",
              "      <td>-0.1080</td>\n",
              "      <td>0.329</td>\n",
              "      <td>-0.0412</td>\n",
              "      <td>0.6810</td>\n",
              "      <td>-0.3930</td>\n",
              "      <td>-0.4570</td>\n",
              "      <td>-0.460</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.2090</td>\n",
              "      <td>1.8500</td>\n",
              "      <td>1.8900</td>\n",
              "      <td>1.8400</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.460</td>\n",
              "      <td>0.534</td>\n",
              "      <td>0.586</td>\n",
              "      <td>1.300</td>\n",
              "      <td>0.3620</td>\n",
              "      <td>-0.2450</td>\n",
              "      <td>-0.2530</td>\n",
              "      <td>2.5700</td>\n",
              "      <td>2.6600</td>\n",
              "      <td>2.7100</td>\n",
              "      <td>-0.574</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.0149</td>\n",
              "      <td>1.2300</td>\n",
              "      <td>0.470</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.040</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.448</td>\n",
              "      <td>0.287</td>\n",
              "      <td>-0.181</td>\n",
              "      <td>-1.090</td>\n",
              "      <td>-1.200</td>\n",
              "      <td>-1.300</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.4070</td>\n",
              "      <td>-0.361</td>\n",
              "      <td>0.571</td>\n",
              "      <td>0.219</td>\n",
              "      <td>-0.0181</td>\n",
              "      <td>0.2480</td>\n",
              "      <td>-0.869</td>\n",
              "      <td>-0.190</td>\n",
              "      <td>0.451</td>\n",
              "      <td>0.6980</td>\n",
              "      <td>0.363</td>\n",
              "      <td>1.030</td>\n",
              "      <td>-0.2490</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.0215</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.5790</td>\n",
              "      <td>0.5020</td>\n",
              "      <td>-0.342</td>\n",
              "      <td>-0.2740</td>\n",
              "      <td>-0.172</td>\n",
              "      <td>-0.164</td>\n",
              "      <td>0.2160</td>\n",
              "      <td>0.0709</td>\n",
              "      <td>1.840</td>\n",
              "      <td>1.770</td>\n",
              "      <td>-0.870</td>\n",
              "      <td>0.271</td>\n",
              "      <td>-0.244</td>\n",
              "      <td>-1.2100</td>\n",
              "      <td>-0.515</td>\n",
              "      <td>0.976</td>\n",
              "      <td>-0.0123</td>\n",
              "      <td>0.614</td>\n",
              "      <td>0.4480</td>\n",
              "      <td>0.9360</td>\n",
              "      <td>-1.610</td>\n",
              "      <td>-0.49500</td>\n",
              "      <td>-0.00127</td>\n",
              "      <td>0.0575</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.1050</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.0811</td>\n",
              "      <td>-0.0721</td>\n",
              "      <td>-0.1600</td>\n",
              "      <td>-0.2800</td>\n",
              "      <td>-0.257</td>\n",
              "      <td>0.279</td>\n",
              "      <td>-0.0382</td>\n",
              "      <td>0.0349</td>\n",
              "      <td>0.0331</td>\n",
              "      <td>-0.0773</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.765</td>\n",
              "      <td>0.371</td>\n",
              "      <td>0.379</td>\n",
              "      <td>0.722</td>\n",
              "      <td>0.2680</td>\n",
              "      <td>-0.2760</td>\n",
              "      <td>-0.2630</td>\n",
              "      <td>0.9480</td>\n",
              "      <td>0.9030</td>\n",
              "      <td>0.8860</td>\n",
              "      <td>-0.477</td>\n",
              "      <td>-0.539</td>\n",
              "      <td>-0.527</td>\n",
              "      <td>-0.5180</td>\n",
              "      <td>0.1480</td>\n",
              "      <td>-0.133</td>\n",
              "      <td>-0.364</td>\n",
              "      <td>0.746</td>\n",
              "      <td>0.4940</td>\n",
              "      <td>-0.180</td>\n",
              "      <td>0.675</td>\n",
              "      <td>0.410</td>\n",
              "      <td>-0.377</td>\n",
              "      <td>-0.340</td>\n",
              "      <td>-0.308</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.0941</td>\n",
              "      <td>0.143</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.363</td>\n",
              "      <td>0.0702</td>\n",
              "      <td>0.0200</td>\n",
              "      <td>0.397</td>\n",
              "      <td>-0.800</td>\n",
              "      <td>0.173</td>\n",
              "      <td>0.7380</td>\n",
              "      <td>0.465</td>\n",
              "      <td>0.440</td>\n",
              "      <td>-0.2880</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-0.2170</td>\n",
              "      <td>-0.3570</td>\n",
              "      <td>-0.0539</td>\n",
              "      <td>-0.0688</td>\n",
              "      <td>0.445</td>\n",
              "      <td>0.6380</td>\n",
              "      <td>0.436</td>\n",
              "      <td>0.351</td>\n",
              "      <td>0.0401</td>\n",
              "      <td>-0.1140</td>\n",
              "      <td>1.310</td>\n",
              "      <td>0.438</td>\n",
              "      <td>0.912</td>\n",
              "      <td>0.209</td>\n",
              "      <td>-0.214</td>\n",
              "      <td>0.0403</td>\n",
              "      <td>-0.554</td>\n",
              "      <td>-0.770</td>\n",
              "      <td>1.0500</td>\n",
              "      <td>0.228</td>\n",
              "      <td>0.2130</td>\n",
              "      <td>-0.0393</td>\n",
              "      <td>-0.132</td>\n",
              "      <td>0.00846</td>\n",
              "      <td>0.96500</td>\n",
              "      <td>0.6010</td>\n",
              "      <td>-0.371</td>\n",
              "      <td>0.0415</td>\n",
              "      <td>-0.320</td>\n",
              "      <td>-0.0412</td>\n",
              "      <td>0.5920</td>\n",
              "      <td>0.0776</td>\n",
              "      <td>1.3100</td>\n",
              "      <td>1.280</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.3950</td>\n",
              "      <td>-0.4660</td>\n",
              "      <td>-0.4460</td>\n",
              "      <td>-0.3570</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.205</td>\n",
              "      <td>-0.833</td>\n",
              "      <td>-0.840</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.0257</td>\n",
              "      <td>0.3970</td>\n",
              "      <td>0.4270</td>\n",
              "      <td>-1.2200</td>\n",
              "      <td>-1.2300</td>\n",
              "      <td>-1.2500</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.176</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.1620</td>\n",
              "      <td>-0.0144</td>\n",
              "      <td>-0.852</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1.270</td>\n",
              "      <td>-0.5040</td>\n",
              "      <td>-0.826</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.474</td>\n",
              "      <td>-0.183</td>\n",
              "      <td>-0.149</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.323</td>\n",
              "      <td>-0.8340</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.157</td>\n",
              "      <td>-0.745</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0622</td>\n",
              "      <td>0.269</td>\n",
              "      <td>-0.217</td>\n",
              "      <td>-1.030</td>\n",
              "      <td>0.0276</td>\n",
              "      <td>0.472</td>\n",
              "      <td>-0.390</td>\n",
              "      <td>0.3660</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-0.0846</td>\n",
              "      <td>0.0166</td>\n",
              "      <td>0.4240</td>\n",
              "      <td>0.3520</td>\n",
              "      <td>-0.259</td>\n",
              "      <td>-0.0947</td>\n",
              "      <td>0.119</td>\n",
              "      <td>-0.162</td>\n",
              "      <td>0.3020</td>\n",
              "      <td>-0.1700</td>\n",
              "      <td>7.670</td>\n",
              "      <td>3.040</td>\n",
              "      <td>-0.991</td>\n",
              "      <td>0.257</td>\n",
              "      <td>-0.239</td>\n",
              "      <td>-0.0800</td>\n",
              "      <td>-0.359</td>\n",
              "      <td>-0.303</td>\n",
              "      <td>0.5330</td>\n",
              "      <td>0.688</td>\n",
              "      <td>0.0567</td>\n",
              "      <td>1.3100</td>\n",
              "      <td>-0.955</td>\n",
              "      <td>-0.17200</td>\n",
              "      <td>0.60700</td>\n",
              "      <td>0.5030</td>\n",
              "      <td>-0.850</td>\n",
              "      <td>-0.5610</td>\n",
              "      <td>-0.782</td>\n",
              "      <td>-0.5170</td>\n",
              "      <td>0.1180</td>\n",
              "      <td>-0.2310</td>\n",
              "      <td>0.0817</td>\n",
              "      <td>0.385</td>\n",
              "      <td>0.260</td>\n",
              "      <td>-0.5360</td>\n",
              "      <td>-0.7020</td>\n",
              "      <td>-0.6550</td>\n",
              "      <td>-0.5090</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.914</td>\n",
              "      <td>-0.656</td>\n",
              "      <td>-0.645</td>\n",
              "      <td>-0.150</td>\n",
              "      <td>-0.2870</td>\n",
              "      <td>-0.1190</td>\n",
              "      <td>-0.1220</td>\n",
              "      <td>-0.0764</td>\n",
              "      <td>-0.0645</td>\n",
              "      <td>-0.0691</td>\n",
              "      <td>-0.155</td>\n",
              "      <td>-1.560</td>\n",
              "      <td>-1.550</td>\n",
              "      <td>-1.5500</td>\n",
              "      <td>-0.4110</td>\n",
              "      <td>-1.030</td>\n",
              "      <td>-0.553</td>\n",
              "      <td>0.495</td>\n",
              "      <td>-0.0884</td>\n",
              "      <td>-1.020</td>\n",
              "      <td>0.588</td>\n",
              "      <td>-0.209</td>\n",
              "      <td>-0.693</td>\n",
              "      <td>-0.683</td>\n",
              "      <td>-0.658</td>\n",
              "      <td>-0.436</td>\n",
              "      <td>-0.7300</td>\n",
              "      <td>-0.801</td>\n",
              "      <td>0.772</td>\n",
              "      <td>-1.010</td>\n",
              "      <td>0.7190</td>\n",
              "      <td>0.3250</td>\n",
              "      <td>-0.286</td>\n",
              "      <td>-0.528</td>\n",
              "      <td>-0.704</td>\n",
              "      <td>0.8530</td>\n",
              "      <td>0.953</td>\n",
              "      <td>-0.116</td>\n",
              "      <td>-0.1190</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 119 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0       1       2       3  ...    115    116     117  type\n",
              "0           0  0.1290 -0.2160  0.2880  ... -1.290 -2.100  0.0121   0.0\n",
              "1           1  0.0989  0.1160  0.3130  ...  0.363  1.030 -0.2490   0.0\n",
              "2           2  0.0215  0.1590  0.5790  ...  0.465  0.440 -0.2880   0.0\n",
              "3           3 -0.2170 -0.3570 -0.0539  ...  0.472 -0.390  0.3660   0.0\n",
              "4           4 -0.0846  0.0166  0.4240  ...  0.953 -0.116 -0.1190   0.0\n",
              "\n",
              "[5 rows x 119 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpOOfNEw3Tc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = data_train['type'].values\n",
        "X_train = data_train.values\n",
        "y_train = y_train.reshape(len(y_train), 1)\n",
        "\n",
        "y_test = data_test['type'].values\n",
        "X_test = data_test.values\n",
        "y_test = y_test.reshape(len(y_test), 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5Yz5Xn43Zfg",
        "colab_type": "code",
        "outputId": "f232c416-1900-4d5b-f661-f1cb210786d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "clf = LogisticRegressionCV(\n",
        "        Cs=list(np.power(10.0, np.arange(-10, 10)))\n",
        "        ,penalty='l2'\n",
        "        ,cv=10\n",
        "        ,random_state=777\n",
        "        ,fit_intercept=True\n",
        "        ,solver='newton-cg'\n",
        "        ,tol=10)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# L2 Regularization parameter\n",
        "print('\\n')\n",
        "print(\"The optimized L2 regularization paramater id:\", clf.C_)\n",
        "\n",
        "# The coefficients\n",
        "print('Estimated beta1: \\n', clf.coef_)\n",
        "print('Estimated beta0: \\n', clf.intercept_)\n",
        "\n",
        "# Metrics\n",
        "print('\\n')\n",
        "print('Test Set Confusion matrix:') \n",
        "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
        "\n",
        "train_score = clf.score(X_train, y_train)\n",
        "test_score = clf.score(X_test, y_test)\n",
        "y_prediction = clf.predict(X_test)\n",
        "test_precision = precision_score(y_test, y_prediction)\n",
        "print('The training classification accuracy is: ', train_score)\n",
        "print('The testing classification accuracy is: ', test_score)\n",
        "print('The precision score on the test set is: ', test_precision)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
            "  % (min_groups, self.n_splits)), Warning)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "The optimized L2 regularization paramater id: [1.e-10]\n",
            "Estimated beta1: \n",
            " [[-1.17582084e-05 -1.78251848e-11 -2.31037356e-11  1.41727627e-10\n",
            "   1.18667386e-10 -2.39443104e-10 -2.27392348e-10 -6.85815192e-11\n",
            "  -8.40707699e-11  8.00345657e-10  5.77402978e-10 -1.72674032e-11\n",
            "  -8.44517369e-12 -2.06842846e-10  6.54724531e-11 -6.54216220e-11\n",
            "  -6.05654990e-11 -2.35674926e-10 -5.53541519e-11  5.87040404e-11\n",
            "   1.83247880e-10  1.90463351e-10  6.92469877e-13  1.06317576e-10\n",
            "   1.56078932e-10  1.85828964e-10  1.57425872e-10  2.28987937e-10\n",
            "  -1.40233048e-10  2.34268161e-10 -1.28060290e-10  2.07606967e-11\n",
            "  -1.88727215e-11  2.92680863e-11  3.32246603e-11  2.46285031e-10\n",
            "   4.33248721e-11 -6.38930162e-11 -5.61976218e-11 -2.60112506e-11\n",
            "  -1.18539171e-10  1.28105005e-10  1.42222999e-10  1.53987852e-10\n",
            "   5.36647178e-11  1.81544231e-10  2.00307440e-10  1.35815878e-10\n",
            "  -2.70292091e-10 -1.51137389e-10 -1.37480733e-10 -1.39855649e-10\n",
            "  -2.97758701e-10 -8.26111521e-11 -3.18105864e-11 -3.57007668e-11\n",
            "  -3.62717812e-11 -3.69089116e-11  3.58231837e-10  3.50801346e-11\n",
            "   3.00839005e-10 -1.36305203e-10  3.06785133e-10  3.20688358e-10\n",
            "   3.30776338e-10  3.37628414e-10 -3.38084881e-10 -2.15083541e-10\n",
            "  -5.15663244e-10 -2.15559132e-10 -2.19632557e-10 -1.35607209e-10\n",
            "  -2.24594691e-10 -7.94572966e-11  6.53290056e-11  6.35977172e-11\n",
            "   6.14675753e-11  1.37628515e-10 -1.79915963e-10  6.93001215e-11\n",
            "  -1.68345307e-10 -1.66612194e-10  4.86093766e-11  9.02532434e-11\n",
            "   2.88438316e-11  2.91993705e-11 -1.25082162e-10 -1.22496550e-10\n",
            "  -1.19348277e-10 -1.87537435e-11 -2.20503015e-10 -2.19925039e-10\n",
            "  -2.19514158e-10 -5.52146465e-11 -6.57147238e-11 -1.53293354e-10\n",
            "  -6.93179327e-11 -1.47779870e-11 -5.54375548e-11  4.61909046e-11\n",
            "  -1.12353970e-12 -1.15669130e-10 -1.06025960e-10 -1.06265193e-10\n",
            "   3.31375682e-11 -3.99825408e-10 -2.25197690e-10 -6.10359155e-11\n",
            "  -2.33561615e-11  1.02989409e-10  9.80037320e-11 -1.61813028e-10\n",
            "   4.52902693e-11 -1.83493600e-10  8.66064154e-11  2.07327461e-10\n",
            "   1.07778635e-12  2.98536568e-11  1.97970231e-10]]\n",
            "Estimated beta0: \n",
            " [-4.43339672]\n",
            "\n",
            "\n",
            "Test Set Confusion matrix:\n",
            "[[325   0]\n",
            " [  1   0]]\n",
            "The training classification accuracy is:  0.9981981981981982\n",
            "The testing classification accuracy is:  0.9969325153374233\n",
            "The precision score on the test set is:  0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCKzMhgA3eUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(9001)\n",
        "path = \"/content/drive/My Drive/DATA SCIENCE/HW6_dataset_missing.csv\"\n",
        "df_2 = pd.read_csv(path)\n",
        "msk = np.random.rand(len(df)) < 0.75\n",
        "data_train_2 = df_2[msk]\n",
        "data_test_2 = df_2[~msk]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub8uG1zT3qZB",
        "colab_type": "code",
        "outputId": "b889a26c-facc-42bf-9398-90dc0b6d87bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for column in data_train_2:\n",
        "    data_train_2[column] = data_train_2[column].fillna(data_train_2[column].mean())\n",
        "for column in data_test_2:\n",
        "    data_test_2[column] = data_test_2[column].fillna(data_train_2[column].mean())\n",
        "    \n",
        "y_train = data_train_2['type'].values\n",
        "X_train = data_train_2.values\n",
        "y_train = y_train.reshape(len(y_train), 1)\n",
        "\n",
        "y_test = data_test_2['type'].values\n",
        "X_test = data_test_2.values\n",
        "y_test = y_test.reshape(len(y_test), 1)\n",
        "\n",
        "\n",
        "# Fit a logistic regression classifier to the training set and report the accuracy of the classifier on the test set\n",
        "clf = LogisticRegressionCV(\n",
        "        Cs=list(np.power(10.0, np.arange(-10, 10)))\n",
        "        ,penalty='l2'\n",
        "        ,cv=10\n",
        "        ,random_state=777\n",
        "        ,fit_intercept=True\n",
        "        ,solver='newton-cg'\n",
        "        ,tol=10)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# L2 Regularization parameter\n",
        "print('\\n')\n",
        "print(\"The optimized L2 regularization paramater id:\", clf.C_)\n",
        "\n",
        "# The coefficients\n",
        "print('Estimated beta1: \\n', clf.coef_)\n",
        "print('Estimated beta0: \\n', clf.intercept_)\n",
        "\n",
        "# Metrics\n",
        "print('\\n')\n",
        "print('Test Set Confusion matrix:') \n",
        "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
        "\n",
        "train_score = clf.score(X_train, y_train)\n",
        "test_score = clf.score(X_test, y_test)\n",
        "y_prediction = clf.predict(X_test)\n",
        "test_precision = precision_score(y_test, y_prediction)\n",
        "print('The training classification accuracy is: ', train_score)\n",
        "print('The testing classification accuracy is: ', test_score)\n",
        "print('The precision score on the test set is: ', test_precision)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "The optimized L2 regularization paramater id: [10.]\n",
            "Estimated beta1: \n",
            " [[-4.03874710e-05 -1.08493517e-01  1.91951351e-01  3.06951454e-01\n",
            "   2.66871499e-01 -2.40489998e-01 -2.49962603e-01  1.04124904e-01\n",
            "   6.41031946e-02  9.95915228e-02 -8.03697048e-02  6.01890594e-02\n",
            "   3.50380145e-02 -1.49235351e-01 -8.60875713e-03 -1.54959335e-01\n",
            "  -2.42828422e-01  3.96731986e-01 -7.80584454e-02 -5.28192013e-02\n",
            "   1.47343386e-01  5.58791868e-02 -1.15350420e-01  1.93672056e-01\n",
            "   5.03522025e-01  2.62928098e-01  1.08961789e-01  4.81023560e-02\n",
            "  -1.25567374e-01 -7.24483109e-02  1.48171131e-01 -1.17115106e-01\n",
            "  -1.36737961e-01  2.03256173e-01  1.55162643e-01  4.90482769e-02\n",
            "   1.36430376e-01 -1.87478387e-01 -1.41059925e-01  1.40920368e-01\n",
            "   1.61930931e-01 -1.39974749e-01 -1.03232045e-01 -1.16590887e-01\n",
            "  -1.10311361e-01  1.58552975e-01  1.32648738e-01  1.38523655e-01\n",
            "  -2.54958007e-01 -7.20944251e-02  2.28413155e-02 -8.76284696e-02\n",
            "   5.01491797e-02  8.24234472e-05 -2.89785487e-02 -2.80986231e-02\n",
            "  -3.02628523e-02 -3.35084759e-02  3.33160364e-01 -1.25225550e-01\n",
            "   5.12655404e-02  5.21949499e-02  5.44125129e-02  8.43659510e-02\n",
            "   1.06291687e-01  1.20141109e-01  2.35238204e-01  9.65043571e-02\n",
            "   1.02632946e-01 -2.13893978e-02 -1.76879079e-02 -7.05279960e-03\n",
            "  -2.41786714e-02 -5.81138517e-02  1.06824298e-01  5.48872374e-02\n",
            "   6.19435571e-03 -1.91815158e-01  2.26423068e-01 -8.68882911e-02\n",
            "   1.13266815e-01  9.32464704e-02 -1.92151752e-01  2.88585563e-01\n",
            "   9.16710685e-03 -2.06587758e-02 -1.49467665e-01 -6.89202885e-02\n",
            "   3.79751046e-02  1.37935487e-01 -5.03377773e-03 -1.32021456e-02\n",
            "  -2.25183152e-02 -1.96023608e-01 -3.64929435e-02  8.53543226e-02\n",
            "   1.34896268e-01  2.63225060e-02  3.53201323e-04 -6.98818002e-02\n",
            "   1.11363446e-01  2.80344007e-01 -7.09028147e-02 -1.77906952e-02\n",
            "  -1.90926696e-01 -2.58445717e-01 -9.94624120e-02 -8.70298295e-02\n",
            "   9.38315933e-02  9.35898178e-02 -7.96901876e-02 -2.26688544e-01\n",
            "  -2.28105406e-01 -3.41347332e-01  1.15977622e-01  2.92687373e-01\n",
            "  -7.73067861e-02  4.93505739e-02  9.81511559e+00]]\n",
            "Estimated beta0: \n",
            " [-8.26203161]\n",
            "\n",
            "\n",
            "Test Set Confusion matrix:\n",
            "[[6074    0]\n",
            " [   3   48]]\n",
            "The training classification accuracy is:  1.0\n",
            "The testing classification accuracy is:  0.9995102040816326\n",
            "The precision score on the test set is:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOfB9VLA5EGy",
        "colab_type": "code",
        "outputId": "12d91471-b5b9-491a-cbf9-281d1d01ebc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "np.random.seed(9001)\n",
        "path = \"/content/drive/My Drive/DATA SCIENCE/HW6_dataset_missing.csv\"\n",
        "df_imp = pd.read_csv(path)\n",
        "msk = np.random.rand(len(df)) < 0.75\n",
        "data_train_imp = df_imp[msk]\n",
        "#print(data_train_imp)\n",
        "data_test_imp = df_imp[~msk]\n",
        "data_train_full = data_train_imp.dropna()\n",
        "\n",
        "data_test_imp.iloc[:, 91]\n",
        "#y_train_imp = data_train['type'].values\n",
        "#X_train_imp = data_train.values\n",
        "#y_train_imp = y_train.reshape(len(y_train), 1)\n",
        "\n",
        "#y_test_imp = data_test_imp['type'].values\n",
        "#X_test_imp = data_test_imp.values\n",
        "#y_test_imp = y_test_imp.reshape(len(y_test), 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1        0.027000\n",
              "2       -0.527000\n",
              "4       -1.550000\n",
              "7        1.270000\n",
              "8        0.571000\n",
              "11       0.128000\n",
              "17       0.424000\n",
              "19      -1.110000\n",
              "20      -0.426000\n",
              "22      -0.681000\n",
              "26      -0.430000\n",
              "29       0.364000\n",
              "30       0.399000\n",
              "31      -0.926000\n",
              "36       0.541000\n",
              "37      -0.924000\n",
              "38       0.650000\n",
              "43       0.708000\n",
              "50       2.390000\n",
              "54       0.825000\n",
              "65       0.430000\n",
              "70      -0.489000\n",
              "71       0.134000\n",
              "74      -0.086000\n",
              "75      -0.553000\n",
              "78      -0.427000\n",
              "79       0.526000\n",
              "87      -0.458000\n",
              "91       0.305000\n",
              "102     -1.230000\n",
              "104     -0.863000\n",
              "105     -1.350000\n",
              "114     -0.500000\n",
              "115      2.490000\n",
              "116     -0.199000\n",
              "119     -2.050000\n",
              "121      0.978000\n",
              "122     -1.390000\n",
              "132     -1.040000\n",
              "135      0.780000\n",
              "136     -0.038100\n",
              "138      1.780000\n",
              "139     -0.586000\n",
              "141     -0.499000\n",
              "143     -0.461000\n",
              "144      0.281000\n",
              "155     -1.850000\n",
              "158     -0.734000\n",
              "160      0.192000\n",
              "164      0.234000\n",
              "168     -0.172000\n",
              "173     -0.782000\n",
              "174     -1.020000\n",
              "175     -0.166000\n",
              "177     -1.020000\n",
              "184     -0.203000\n",
              "202     -0.489000\n",
              "205     -0.614000\n",
              "206     -0.442000\n",
              "208      0.778000\n",
              "209     -0.246000\n",
              "210      0.677000\n",
              "211     -1.470000\n",
              "219     -1.140000\n",
              "220     -0.561000\n",
              "225     -0.622000\n",
              "227      0.575000\n",
              "230      3.130000\n",
              "231     -0.185000\n",
              "238     -0.369000\n",
              "243     -0.847000\n",
              "244     -0.460000\n",
              "255     -0.703000\n",
              "259     -0.620000\n",
              "261     -0.770000\n",
              "267     -0.421000\n",
              "271      0.248000\n",
              "274      1.740000\n",
              "275      1.850000\n",
              "277     -2.340000\n",
              "279     -0.168000\n",
              "282      0.781000\n",
              "283     -0.557000\n",
              "288     -0.274000\n",
              "300      0.058200\n",
              "305      0.406000\n",
              "306     -0.158000\n",
              "313     -0.535000\n",
              "321     -0.278000\n",
              "326     -0.928000\n",
              "327     -0.633000\n",
              "329     -0.424000\n",
              "330      0.040100\n",
              "333     -1.050000\n",
              "335      1.960000\n",
              "336      0.591000\n",
              "337      0.981000\n",
              "339     -1.800000\n",
              "341      0.567000\n",
              "345      0.048600\n",
              "350      0.401000\n",
              "358      3.190000\n",
              "363      0.252000\n",
              "369     -2.840000\n",
              "370     -0.134000\n",
              "371     -0.924000\n",
              "378     -0.758000\n",
              "380     -1.060000\n",
              "381      0.408000\n",
              "382     -0.283000\n",
              "386      0.512000\n",
              "392      0.497000\n",
              "396      3.080000\n",
              "401     -1.050000\n",
              "407      2.040000\n",
              "412     -0.774000\n",
              "416     -0.539000\n",
              "417     -0.533000\n",
              "421     -0.194000\n",
              "429      0.295000\n",
              "437     -0.904000\n",
              "445     -1.400000\n",
              "446      0.411000\n",
              "449      0.295000\n",
              "454     -1.080000\n",
              "455     -0.107000\n",
              "458     -0.448000\n",
              "460     -0.337000\n",
              "467      2.770000\n",
              "472     -1.260000\n",
              "473      0.256000\n",
              "487      2.150000\n",
              "489     -0.987000\n",
              "492     -0.749000\n",
              "493      0.251000\n",
              "494     -0.423000\n",
              "496     -1.460000\n",
              "497     -0.358000\n",
              "511     -0.688000\n",
              "514      0.453000\n",
              "518     -0.586000\n",
              "522      0.870000\n",
              "525     -0.386000\n",
              "530      0.774000\n",
              "535      0.765000\n",
              "536      1.830000\n",
              "537      0.652000\n",
              "539     -0.422000\n",
              "540     -0.192000\n",
              "548      0.067000\n",
              "551      1.780000\n",
              "552      0.465000\n",
              "553     -0.053300\n",
              "555      0.322000\n",
              "566      0.169000\n",
              "572      0.787000\n",
              "575     -0.061300\n",
              "586     -0.093100\n",
              "587      0.947000\n",
              "602     -0.513000\n",
              "603     -0.342000\n",
              "605     -0.349000\n",
              "607     -0.173000\n",
              "609     -0.257000\n",
              "610     -0.542000\n",
              "615      1.000000\n",
              "618     -0.761000\n",
              "623      1.170000\n",
              "626      1.410000\n",
              "629      0.740000\n",
              "639      0.405000\n",
              "640     -0.689000\n",
              "657      0.666000\n",
              "663      0.465000\n",
              "667     -1.080000\n",
              "668     -0.355000\n",
              "669     -0.579000\n",
              "681     -0.131000\n",
              "684      0.427000\n",
              "690      1.040000\n",
              "691      1.280000\n",
              "699      0.238000\n",
              "701      1.130000\n",
              "716     -1.290000\n",
              "717      2.260000\n",
              "730      0.809000\n",
              "731     -0.973000\n",
              "733     -0.847000\n",
              "741      0.547000\n",
              "742     -1.290000\n",
              "749     -0.484000\n",
              "754     -0.923000\n",
              "755      0.213000\n",
              "756      0.224000\n",
              "762     -2.090000\n",
              "763      1.050000\n",
              "771      0.223000\n",
              "775      0.832000\n",
              "782     -0.581000\n",
              "784     -2.130000\n",
              "788     -0.152000\n",
              "789      0.157000\n",
              "793      0.960000\n",
              "794      0.412000\n",
              "795      0.155000\n",
              "798      0.963000\n",
              "801      0.048300\n",
              "802     -0.979000\n",
              "805     -0.260000\n",
              "812      0.308000\n",
              "814     -0.110000\n",
              "819      0.899000\n",
              "826     -0.690000\n",
              "828     -0.846000\n",
              "830      0.325000\n",
              "831     -1.580000\n",
              "835      0.641000\n",
              "836      0.980000\n",
              "838      0.076500\n",
              "840      1.190000\n",
              "843      2.190000\n",
              "849     -2.250000\n",
              "850     -0.550000\n",
              "851      0.390000\n",
              "858      0.461000\n",
              "876     -1.280000\n",
              "883     -0.669000\n",
              "889     -0.413000\n",
              "891     -0.715000\n",
              "892     -0.723000\n",
              "897     -0.287000\n",
              "898      0.749000\n",
              "899      0.084800\n",
              "903      0.767000\n",
              "907      0.094500\n",
              "908     -0.164000\n",
              "909     -0.814000\n",
              "915     -0.144000\n",
              "920      1.240000\n",
              "921      0.851000\n",
              "927     -0.647000\n",
              "936     -0.223000\n",
              "939     -0.435000\n",
              "944     -0.086500\n",
              "948      0.021300\n",
              "949      0.328000\n",
              "950      0.033400\n",
              "952      0.276000\n",
              "953     -0.390000\n",
              "955      2.080000\n",
              "959      0.296000\n",
              "960     -0.292000\n",
              "963      0.492000\n",
              "965      0.637000\n",
              "973      0.012600\n",
              "975      0.556000\n",
              "978     -0.909000\n",
              "980     -1.010000\n",
              "981     -0.413000\n",
              "982      0.300000\n",
              "989     -0.746000\n",
              "993      0.146000\n",
              "994     -0.609000\n",
              "999     -0.182000\n",
              "1000     0.153000\n",
              "1005     0.265000\n",
              "1008     0.069500\n",
              "1012     0.232000\n",
              "1016    -0.406000\n",
              "1020    -1.050000\n",
              "1021    -0.266000\n",
              "1027    -0.455000\n",
              "1030    -1.680000\n",
              "1036     0.214000\n",
              "1039    -1.630000\n",
              "1042     0.102000\n",
              "1043    -0.822000\n",
              "1046     1.730000\n",
              "1054    -0.858000\n",
              "1065     1.700000\n",
              "1076     1.130000\n",
              "1078     1.370000\n",
              "1080    -0.651000\n",
              "1090     0.362000\n",
              "1091    -0.570000\n",
              "1092     0.284000\n",
              "1100    -1.210000\n",
              "1103     1.000000\n",
              "1108     1.120000\n",
              "1109    -0.529000\n",
              "1110     0.036900\n",
              "1111    -0.965000\n",
              "1112    -0.101000\n",
              "1114     1.330000\n",
              "1127    -0.541000\n",
              "1134     0.782000\n",
              "1136    -0.467000\n",
              "1140     0.152000\n",
              "1149     0.000445\n",
              "1158    -0.916000\n",
              "1163     0.930000\n",
              "1173     0.197000\n",
              "1176    -1.000000\n",
              "1181    -0.320000\n",
              "1183     0.936000\n",
              "1185    -0.865000\n",
              "1186     2.170000\n",
              "1187    -0.740000\n",
              "1190     0.776000\n",
              "1198     1.050000\n",
              "1200    -0.865000\n",
              "1209    -0.186000\n",
              "1211     1.150000\n",
              "1219    -0.579000\n",
              "1224     4.670000\n",
              "1227     0.417000\n",
              "1231     0.100000\n",
              "1238    -0.510000\n",
              "1245    -0.096100\n",
              "1247    -0.205000\n",
              "1249    -1.680000\n",
              "1250    -0.257000\n",
              "1258     0.978000\n",
              "1260     1.020000\n",
              "1261     0.232000\n",
              "1267     0.049000\n",
              "1270    -2.280000\n",
              "1271     0.421000\n",
              "1273     3.260000\n",
              "1279    -0.419000\n",
              "1281     1.470000\n",
              "1284    -1.060000\n",
              "1300    -0.013400\n",
              "1305    -0.064500\n",
              "1306     0.197000\n",
              "1307     0.091600\n",
              "1313     1.790000\n",
              "1319     0.991000\n",
              "1327    -0.298000\n",
              "1333    -0.764000\n",
              "1346    -0.004520\n",
              "1348    -2.010000\n",
              "1353     0.252000\n",
              "1362     0.635000\n",
              "1364    -0.799000\n",
              "1367     1.020000\n",
              "1368     0.212000\n",
              "1377    -0.436000\n",
              "1379    -0.625000\n",
              "1388    -0.388000\n",
              "1395    -0.609000\n",
              "1417    -0.923000\n",
              "1420    -0.149000\n",
              "1425    -0.367000\n",
              "1426     0.659000\n",
              "1428     1.830000\n",
              "1437     0.008270\n",
              "1443     0.049800\n",
              "1445    -0.926000\n",
              "1453    -0.701000\n",
              "1454    -0.599000\n",
              "1457     0.321000\n",
              "1459    -0.500000\n",
              "1460    -0.275000\n",
              "1461     0.875000\n",
              "1464     1.260000\n",
              "1469    -0.507000\n",
              "1472    -0.667000\n",
              "1473     0.044500\n",
              "1476    -0.844000\n",
              "1480    -0.201000\n",
              "1481    -0.963000\n",
              "1484     0.706000\n",
              "1485     0.104000\n",
              "1490    -0.996000\n",
              "1492    -0.413000\n",
              "1494     1.480000\n",
              "1495    -0.648000\n",
              "1509     0.795000\n",
              "1526    -0.285000\n",
              "1528    -0.206000\n",
              "1529     0.388000\n",
              "1531    -0.282000\n",
              "1533     0.994000\n",
              "1535     1.550000\n",
              "1542    -1.440000\n",
              "1543    -0.079900\n",
              "1545    -0.598000\n",
              "1549     0.694000\n",
              "1550    -0.062900\n",
              "1553    -1.720000\n",
              "1558     0.443000\n",
              "1560    -0.401000\n",
              "1563    -0.085800\n",
              "1565     0.131000\n",
              "1573    -0.982000\n",
              "1578    -0.028500\n",
              "1580     1.370000\n",
              "1584     0.381000\n",
              "1587    -0.257000\n",
              "1588    -0.240000\n",
              "1590    -0.473000\n",
              "1596    -1.340000\n",
              "1597    -1.220000\n",
              "1598     0.104000\n",
              "1600    -0.155000\n",
              "1609     0.101000\n",
              "1610    -0.136000\n",
              "1614     0.999000\n",
              "1615    -0.586000\n",
              "1616    -0.830000\n",
              "1620    -0.430000\n",
              "1623     0.682000\n",
              "1629     1.150000\n",
              "1631    -0.477000\n",
              "1636     1.910000\n",
              "1641    -0.835000\n",
              "1644     0.860000\n",
              "1649    -1.280000\n",
              "1651     0.293000\n",
              "1660     0.753000\n",
              "1664    -0.669000\n",
              "1669     0.066200\n",
              "1674     0.783000\n",
              "1675     0.745000\n",
              "1676     0.983000\n",
              "1678    -0.938000\n",
              "1679    -0.234000\n",
              "1683    -0.553000\n",
              "1684    -0.553000\n",
              "1685    -0.360000\n",
              "1686    -0.386000\n",
              "1697    -0.982000\n",
              "1699     1.070000\n",
              "1703     0.526000\n",
              "1706     0.530000\n",
              "1709    -0.731000\n",
              "1710    -0.191000\n",
              "1716     0.089400\n",
              "1717    -0.254000\n",
              "1725     0.579000\n",
              "1730    -0.860000\n",
              "1732     0.507000\n",
              "1735    -0.395000\n",
              "1737    -0.718000\n",
              "1740     0.739000\n",
              "1747     1.100000\n",
              "1751     0.291000\n",
              "1752     1.920000\n",
              "1754     1.280000\n",
              "1756     0.178000\n",
              "1769     0.202000\n",
              "1780     0.029000\n",
              "1783    -0.586000\n",
              "1799    -1.180000\n",
              "1800     0.291000\n",
              "1806    -0.973000\n",
              "1807    -0.691000\n",
              "1808     1.050000\n",
              "1813    -0.700000\n",
              "1815    -0.128000\n",
              "1818     0.225000\n",
              "1822     0.709000\n",
              "1823    -0.313000\n",
              "1829     0.997000\n",
              "1837    -0.650000\n",
              "1842     0.357000\n",
              "1850    -0.403000\n",
              "1852    -0.481000\n",
              "1853    -0.589000\n",
              "1856    -0.957000\n",
              "1862    -1.220000\n",
              "1874    -0.241000\n",
              "1890    -1.290000\n",
              "1892    -0.761000\n",
              "1893     0.205000\n",
              "1898    -0.654000\n",
              "1899     0.816000\n",
              "1901     0.032600\n",
              "1908     1.670000\n",
              "1910    -0.971000\n",
              "1918     0.362000\n",
              "1920     0.496000\n",
              "1922     2.350000\n",
              "1927     0.058300\n",
              "1936     0.713000\n",
              "1946    -0.698000\n",
              "1947    -0.060500\n",
              "1948    -0.341000\n",
              "1950    -1.260000\n",
              "1956    -0.117000\n",
              "1961    -1.080000\n",
              "1967    -1.290000\n",
              "1972     0.565000\n",
              "1975    -0.482000\n",
              "1983    -0.932000\n",
              "1986    -0.131000\n",
              "1991    -0.232000\n",
              "1992     0.451000\n",
              "1994    -0.047200\n",
              "           ...   \n",
              "22904   -0.097900\n",
              "22905    0.639000\n",
              "22907    0.340000\n",
              "22919   -0.099100\n",
              "22921    0.332000\n",
              "22927    0.177000\n",
              "22928    1.180000\n",
              "22935    0.130000\n",
              "22939    0.440000\n",
              "22943   -0.372000\n",
              "22944    0.123000\n",
              "22946   -0.363000\n",
              "22949   -0.236000\n",
              "22958   -0.901000\n",
              "22965    0.498000\n",
              "22970   -0.403000\n",
              "22975   -1.120000\n",
              "22978   -1.220000\n",
              "22979   -0.933000\n",
              "22985   -0.711000\n",
              "22998    0.862000\n",
              "23001   -0.200000\n",
              "23013   -1.000000\n",
              "23020    1.620000\n",
              "23032    1.420000\n",
              "23037   -1.160000\n",
              "23046    0.958000\n",
              "23047    0.991000\n",
              "23049   -0.517000\n",
              "23051   -0.290000\n",
              "23052   -0.326000\n",
              "23054   -0.775000\n",
              "23067   -0.135000\n",
              "23070   -0.592000\n",
              "23071    1.960000\n",
              "23079   -0.585000\n",
              "23097    2.050000\n",
              "23102    0.513000\n",
              "23106   -0.900000\n",
              "23114    0.693000\n",
              "23117    1.000000\n",
              "23121   -1.270000\n",
              "23133   -0.777000\n",
              "23136   -1.660000\n",
              "23145   -0.439000\n",
              "23147    0.735000\n",
              "23148    0.797000\n",
              "23164    0.402000\n",
              "23166   -1.080000\n",
              "23169   -0.085100\n",
              "23171   -0.526000\n",
              "23177    0.824000\n",
              "23187    1.360000\n",
              "23193    0.203000\n",
              "23194   -1.000000\n",
              "23197   -0.470000\n",
              "23205    1.290000\n",
              "23206    0.217000\n",
              "23209   -0.796000\n",
              "23211    0.313000\n",
              "23213    0.172000\n",
              "23216   -0.946000\n",
              "23218   -0.377000\n",
              "23222   -0.323000\n",
              "23225   -0.537000\n",
              "23236   -0.453000\n",
              "23238   -0.292000\n",
              "23241   -0.715000\n",
              "23244    2.540000\n",
              "23249    0.188000\n",
              "23250   -0.928000\n",
              "23252   -1.280000\n",
              "23253   -0.308000\n",
              "23267    2.110000\n",
              "23269   -0.705000\n",
              "23271   -0.838000\n",
              "23278   -0.611000\n",
              "23281    0.882000\n",
              "23282   -0.371000\n",
              "23291   -0.930000\n",
              "23293   -0.124000\n",
              "23295   -0.664000\n",
              "23298   -0.927000\n",
              "23308   -0.589000\n",
              "23311   -0.829000\n",
              "23315   -0.268000\n",
              "23316    0.173000\n",
              "23325   -0.720000\n",
              "23341   -1.220000\n",
              "23347   -0.468000\n",
              "23357    1.270000\n",
              "23369   -0.839000\n",
              "23372   -1.020000\n",
              "23376   -0.366000\n",
              "23381    0.537000\n",
              "23384   -0.468000\n",
              "23392   -0.332000\n",
              "23394   -0.301000\n",
              "23399    0.259000\n",
              "23400    0.302000\n",
              "23410    0.593000\n",
              "23414   -0.892000\n",
              "23419   -0.454000\n",
              "23423   -0.667000\n",
              "23424   -0.854000\n",
              "23426    0.840000\n",
              "23431   -0.448000\n",
              "23432    0.726000\n",
              "23435    0.679000\n",
              "23441   -2.430000\n",
              "23442   -2.080000\n",
              "23443    0.142000\n",
              "23444   -1.830000\n",
              "23446    0.104000\n",
              "23450    0.285000\n",
              "23451   -0.429000\n",
              "23455   -0.897000\n",
              "23457    0.492000\n",
              "23458    1.770000\n",
              "23467   -0.645000\n",
              "23471   -0.559000\n",
              "23473    2.200000\n",
              "23487    1.340000\n",
              "23493   -0.583000\n",
              "23501    0.604000\n",
              "23509   -0.682000\n",
              "23515   -0.761000\n",
              "23516   -1.390000\n",
              "23521   -2.340000\n",
              "23523   -0.353000\n",
              "23537   -0.426000\n",
              "23542   -0.146000\n",
              "23545    0.822000\n",
              "23555    0.493000\n",
              "23563   -1.100000\n",
              "23573   -0.205000\n",
              "23590    0.519000\n",
              "23593    1.350000\n",
              "23597   -1.260000\n",
              "23598   -0.390000\n",
              "23602   -0.592000\n",
              "23605   -1.530000\n",
              "23606   -0.095000\n",
              "23609   -0.316000\n",
              "23616    0.552000\n",
              "23617   -0.108000\n",
              "23625    0.508000\n",
              "23627   -0.673000\n",
              "23633   -0.230000\n",
              "23645   -0.517000\n",
              "23647    0.440000\n",
              "23655    4.090000\n",
              "23656   -0.516000\n",
              "23663    0.363000\n",
              "23664   -0.301000\n",
              "23666   -0.691000\n",
              "23670   -0.595000\n",
              "23675   -0.843000\n",
              "23679    0.032700\n",
              "23683    1.870000\n",
              "23688   -0.398000\n",
              "23691   -1.430000\n",
              "23699   -0.111000\n",
              "23706   -0.541000\n",
              "23707    1.190000\n",
              "23712   -0.516000\n",
              "23713   -0.595000\n",
              "23714    0.204000\n",
              "23715   -0.606000\n",
              "23719   -0.470000\n",
              "23720    1.300000\n",
              "23724    0.058400\n",
              "23725    0.424000\n",
              "23727    0.209000\n",
              "23728   -0.785000\n",
              "23729   -1.050000\n",
              "23730    1.770000\n",
              "23737   -1.150000\n",
              "23738    0.128000\n",
              "23744    0.361000\n",
              "23745   -1.430000\n",
              "23747   -1.520000\n",
              "23748   -0.738000\n",
              "23751    0.178000\n",
              "23763    1.020000\n",
              "23767    0.030900\n",
              "23769   -0.116000\n",
              "23771    0.210000\n",
              "23772    0.146000\n",
              "23778    0.000591\n",
              "23780    0.901000\n",
              "23781    0.447000\n",
              "23787   -0.272000\n",
              "23788   -0.369000\n",
              "23799   -0.801000\n",
              "23803   -0.511000\n",
              "23805    0.824000\n",
              "23810    0.022000\n",
              "23818   -0.993000\n",
              "23819   -0.262000\n",
              "23827   -0.463000\n",
              "23830    0.531000\n",
              "23831   -1.100000\n",
              "23834   -0.833000\n",
              "23847   -0.096300\n",
              "23850    0.342000\n",
              "23853   -1.620000\n",
              "23854   -0.540000\n",
              "23855    0.286000\n",
              "23856   -0.376000\n",
              "23860   -1.320000\n",
              "23866   -0.126000\n",
              "23868   -0.459000\n",
              "23873    0.104000\n",
              "23884   -0.905000\n",
              "23886   -0.178000\n",
              "23890    1.110000\n",
              "23893   -0.051100\n",
              "23895    0.751000\n",
              "23897   -1.920000\n",
              "23899   -0.318000\n",
              "23901    0.717000\n",
              "23903   -0.520000\n",
              "23906    0.231000\n",
              "23907    0.126000\n",
              "23921    4.950000\n",
              "23932    0.944000\n",
              "23933   -0.241000\n",
              "23938   -0.485000\n",
              "23940   -0.518000\n",
              "23946    0.399000\n",
              "23951    0.939000\n",
              "23955   -1.100000\n",
              "23957   -0.568000\n",
              "23959   -0.169000\n",
              "23963   -0.008430\n",
              "23965   -0.352000\n",
              "23969   -0.997000\n",
              "23971    0.149000\n",
              "23975    0.526000\n",
              "23976    0.885000\n",
              "23978   -1.520000\n",
              "23983   -1.090000\n",
              "23994   -0.175000\n",
              "23995    0.955000\n",
              "23998    1.370000\n",
              "24000    0.414000\n",
              "24005   -0.190000\n",
              "24008   -0.993000\n",
              "24011    0.483000\n",
              "24014   -0.720000\n",
              "24016   -0.562000\n",
              "24024   -0.684000\n",
              "24031   -0.890000\n",
              "24035   -0.233000\n",
              "24042    0.067900\n",
              "24047    0.516000\n",
              "24054   -0.400000\n",
              "24057   -0.150000\n",
              "24058    0.853000\n",
              "24059   -0.578000\n",
              "24060   -0.749000\n",
              "24064    2.060000\n",
              "24072    0.850000\n",
              "24073   -0.202000\n",
              "24075    1.920000\n",
              "24080   -0.518000\n",
              "24088    0.006650\n",
              "24089   -0.504000\n",
              "24090   -1.160000\n",
              "24097   -0.600000\n",
              "24098   -0.272000\n",
              "24106    0.425000\n",
              "24111    1.630000\n",
              "24114   -0.536000\n",
              "24115    1.370000\n",
              "24122    1.570000\n",
              "24123    0.577000\n",
              "24133   -1.580000\n",
              "24135   -0.452000\n",
              "24140   -0.054300\n",
              "24141   -0.332000\n",
              "24142   -0.254000\n",
              "24144   -0.113000\n",
              "24146    0.710000\n",
              "24147    1.530000\n",
              "24148    0.294000\n",
              "24151    0.288000\n",
              "24153   -0.734000\n",
              "24154    0.446000\n",
              "24156   -0.508000\n",
              "24158    3.740000\n",
              "24165   -0.682000\n",
              "24168    0.751000\n",
              "24173   -0.476000\n",
              "24182    0.493000\n",
              "24183   -1.130000\n",
              "24187   -0.394000\n",
              "24193    1.780000\n",
              "24196   -0.842000\n",
              "24199   -0.687000\n",
              "24200   -0.886000\n",
              "24203    0.303000\n",
              "24208    2.600000\n",
              "24209    0.189000\n",
              "24217   -1.540000\n",
              "24229   -0.100000\n",
              "24230   -0.923000\n",
              "24232    1.740000\n",
              "24236   -0.747000\n",
              "24240   -0.162000\n",
              "24244   -0.146000\n",
              "24247   -0.085900\n",
              "24252    1.600000\n",
              "24253   -1.170000\n",
              "24256   -0.420000\n",
              "24261   -0.710000\n",
              "24268    0.471000\n",
              "24269   -0.160000\n",
              "24270    0.509000\n",
              "24275    0.915000\n",
              "24284   -0.886000\n",
              "24286   -0.411000\n",
              "24288    3.450000\n",
              "24293    0.944000\n",
              "24307   -0.657000\n",
              "24310   -0.189000\n",
              "24316    3.570000\n",
              "24321   -0.744000\n",
              "24322   -0.048800\n",
              "24327   -0.248000\n",
              "24328    1.500000\n",
              "24330    0.042400\n",
              "24331   -0.410000\n",
              "24332    1.580000\n",
              "24338   -0.109000\n",
              "24344    1.300000\n",
              "24351    0.116000\n",
              "24352   -0.523000\n",
              "24356   -0.087700\n",
              "24365    0.616000\n",
              "24366    0.436000\n",
              "24368   -0.137000\n",
              "24369   -0.410000\n",
              "24377   -0.708000\n",
              "24395   -1.230000\n",
              "24396    0.757000\n",
              "24397   -0.307000\n",
              "24398   -1.250000\n",
              "24403   -0.951000\n",
              "24405    0.559000\n",
              "24406   -0.399000\n",
              "24407   -0.251000\n",
              "24413   -0.687000\n",
              "24417    0.628000\n",
              "24421   -2.810000\n",
              "24428   -0.616000\n",
              "24429    0.443000\n",
              "24432    0.233000\n",
              "24437   -0.502000\n",
              "24439   -0.137000\n",
              "24441    0.054600\n",
              "24443    0.435000\n",
              "24450   -0.166000\n",
              "24462   -1.260000\n",
              "24463   -0.215000\n",
              "24466    0.144000\n",
              "24467   -1.750000\n",
              "24469   -0.747000\n",
              "24475    1.260000\n",
              "24476    0.057300\n",
              "24480    1.320000\n",
              "24496   -0.834000\n",
              "24514    0.928000\n",
              "24517   -0.923000\n",
              "24518   -1.140000\n",
              "24532    0.341000\n",
              "24533    0.244000\n",
              "24535   -0.166000\n",
              "24538    2.510000\n",
              "24543   -0.863000\n",
              "24544   -0.151000\n",
              "24545    0.483000\n",
              "24547    0.385000\n",
              "24551   -0.502000\n",
              "24554   -0.691000\n",
              "24560    1.890000\n",
              "24564    0.098600\n",
              "24565    0.717000\n",
              "24567   -1.170000\n",
              "24568   -1.240000\n",
              "24571    0.477000\n",
              "24580    0.502000\n",
              "24583   -0.040200\n",
              "24584   -0.006010\n",
              "24586   -0.051300\n",
              "24587   -0.618000\n",
              "24591   -0.877000\n",
              "24592   -1.200000\n",
              "24600    0.649000\n",
              "24604   -1.000000\n",
              "24606   -0.317000\n",
              "24608    0.866000\n",
              "24616   -0.112000\n",
              "24624   -0.233000\n",
              "24637    0.529000\n",
              "24643   -0.537000\n",
              "24644   -1.540000\n",
              "24645   -0.954000\n",
              "24651   -1.250000\n",
              "24652    1.370000\n",
              "24653   -0.018900\n",
              "24655    0.110000\n",
              "24662   -0.375000\n",
              "24670   -1.290000\n",
              "24673    3.560000\n",
              "24674   -0.361000\n",
              "24682    1.060000\n",
              "24699   -0.777000\n",
              "24700   -0.482000\n",
              "24701   -1.170000\n",
              "24702    0.001850\n",
              "24704    0.440000\n",
              "24706    0.459000\n",
              "24711    0.184000\n",
              "24712   -1.930000\n",
              "24715   -1.560000\n",
              "24719    0.425000\n",
              "24724   -0.346000\n",
              "24733   -0.149000\n",
              "24736   -0.078900\n",
              "24739   -0.333000\n",
              "24747    4.180000\n",
              "24748    1.330000\n",
              "24750   -0.006220\n",
              "24757   -0.252000\n",
              "24759    0.643000\n",
              "24763   -1.400000\n",
              "24764   -0.892000\n",
              "24765   -0.294000\n",
              "24767    0.098300\n",
              "24769   -0.037600\n",
              "24771   -0.348000\n",
              "24777    1.790000\n",
              "24780   -0.655000\n",
              "24781   -0.606000\n",
              "24783   -0.215000\n",
              "24788    0.303000\n",
              "24790    0.036300\n",
              "24792   -0.617000\n",
              "24794   -0.344000\n",
              "24798   -0.559000\n",
              "24808    0.337000\n",
              "24812   -0.967000\n",
              "24820   -0.913000\n",
              "24823   -0.655000\n",
              "24824    0.101000\n",
              "24827   -0.250000\n",
              "24834   -0.031300\n",
              "24835    2.490000\n",
              "24838    1.050000\n",
              "24839   -0.921000\n",
              "24844   -0.354000\n",
              "24845    0.365000\n",
              "24862   -1.470000\n",
              "24867   -0.619000\n",
              "24868    0.341000\n",
              "24873    2.800000\n",
              "24882   -0.207000\n",
              "24884   -0.602000\n",
              "24891   -0.246000\n",
              "24892   -0.523000\n",
              "24893    0.350000\n",
              "24895    0.320000\n",
              "24907   -0.279000\n",
              "24913   -0.736000\n",
              "24916   -0.641000\n",
              "24919   -0.138000\n",
              "24925   -0.272000\n",
              "24930   -0.151000\n",
              "24931    0.299000\n",
              "24939    3.570000\n",
              "24940   -0.410000\n",
              "24942   -0.071700\n",
              "24945   -0.183000\n",
              "24950   -1.010000\n",
              "24957   -1.070000\n",
              "24959    1.650000\n",
              "24961   -0.318000\n",
              "24965    0.376000\n",
              "24967   -0.089700\n",
              "24970   -0.249000\n",
              "24971   -0.308000\n",
              "24976   -0.188000\n",
              "24977    0.646000\n",
              "24980    0.100000\n",
              "24988   -0.634000\n",
              "24989   -0.194000\n",
              "24993   -1.930000\n",
              "24998    1.260000\n",
              "Name: 91, Length: 6125, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45TeDHhnJDxs",
        "colab_type": "text"
      },
      "source": [
        "## APCOMP209a - Homework Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "M03HoX-AJDxt",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "This problem walks you through the derivation of the **likelihood equations** for a generalized linear model (GLM). Suppose that the random component of the GLM is in the univariate natural exponential family, so that\n",
        "$$f(y_i|\\theta_i) = h(y_i) e^{y_i\\theta_i - b(\\theta_i)}$$\n",
        "Define the individual log-likelihood for each observation $i$ as\n",
        "$$l_i(\\theta_i) \\equiv \\log f(y_i|\\theta_i)$$\n",
        "with linear predictor\n",
        "$$\\eta_i = x_i^T\\beta = g(\\mu_i)$$\n",
        "for some link function $g$ and where $\\mu_i=E(Y_i)$.\n",
        "\n",
        "1. Use the above expressions to write a simplified expression for the log-likelihood $l(\\theta)$ for the entire dataset, $y_1, \\dots, y_n$.\n",
        "\n",
        "2. Use the chain rule to express $\\frac{\\partial l_i}{\\partial \\beta_j}$ in terms of the derivatives of $l_i, \\theta_i, \\mu_i$, and $\\eta_i$. (*Hint*: Think carefully about which variables are related to which, and in what way. For example, for which of the above variables do you know the derivative with respect to $\\beta_j$?)\n",
        "\n",
        "3. Compute the derivatives for $\\frac{\\partial l_i}{\\partial \\theta_i}$ and $\\frac{\\partial \\eta_i}{\\partial \\beta_j}$.\n",
        "\n",
        "4. Express $\\mu_i$ in terms of $\\theta_i$, and use this relationship to compute $\\frac{\\partial \\theta_i}{\\partial \\mu_i}$. (\\emph{Hint}: Recall the cumulant function of a natural exponential family, and assume that you can write $\\partial f/\\partial g = (\\partial g / \\partial f)^{-1}$.)\n",
        "\n",
        "5. Express $\\eta_i$ in terms of $\\mu_i$. Using the same hint as the above, compute $\\frac{\\partial \\mu_i}{\\partial \\eta_i}$.\n",
        "\n",
        "6. Put all of the above parts together to write an expression for $\\frac{\\partial l}{\\partial \\beta_j}$. Use matrix notation to write this expression as\n",
        "$$\\nabla_{\\beta} l(\\beta) = XDV^{-1}(Y - \\mu) = 0$$\n",
        "That is, compute the matrices $D$ and $V$ such that this equation holds.\n",
        "\n",
        "7. If we use the canonical link function, how do your answers to part (6) simplify?\n",
        "\n",
        "8. Finally, compute the above likelihood equations in the case of logistic regression, and show that this is equivalent to the solution given in lecture.\n"
      ]
    }
  ]
}